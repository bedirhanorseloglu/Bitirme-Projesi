# -*- coding: utf-8 -*-
"""13- Bitirme_Projesi_Deneme13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aA8LAA9kg645erhtFtvhwDoXuiPCgAuF

# 1- Drive Bağlantısı
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 2- Kütüphaneler"""

!pip install pymap3d pyproj

# 1. Matematik ve Sayısal İşlemler
import math
import numpy as np
import scipy.optimize # Optimizasyon işlemleri için
from scipy.spatial import distance # İki nokta arasındaki mesafeleri hesaplamak için.
from scipy import signal #  Sinyal işleme fonksiyonları için
from scipy.interpolate import InterpolatedUnivariateSpline # Veriler üzerinde spline interpolasyonu yapmak için. Yani; noktalar arasında düzgün, kesintisiz ve pürüzsüz eğriler oluşturur.

# 2. Görselleştirme ve Veri Analizi
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px # Etkileşimli ve daha gelişmiş görselleştirmeler için.
import plotly.graph_objects as go # Etkileşimli ve daha gelişmiş görselleştirmeler için.

# 3. Coğrafi Hesaplamalar
import pymap3d as pm # GNSS konum verilerinin dönüştürülmesi ve Vincenty formülleri ile mesafe hesaplamaları için.
import pymap3d.vincenty as pmv #GNSS konum verilerinin dönüştürülmesi ve Vincenty formülleri ile mesafe hesaplamaları için.
import pyproj as proj # Coğrafi projeksiyonlar ve dönüşümler için.

# 4. Makine Öğrenimi ve Derin Öğrenme
from tensorflow import keras # Derin öğrenme modelleri oluşturmak ve eğitmek için.
from keras import layers # Yapay sinir ağları katmanlarını tanımlamak ve oluşturmak için .
from keras import models # Yapay sinir ağları modellerini tanımlamak ve oluşturmak için.

# 5. Dosya ve İlerleme Takibi
import glob as gl # Dosya sistemi ile çalışmak ve belirli desenlere uyan dosyaları bulmak için.
from tqdm.auto import tqdm # Uzun süren işlemlerde  ilerleme çubuğu oluşturmak için.

# 6. Gereksiz Uyarıları Görmeme
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Sabitler
CLIGHT = 299_792_458   # Işık Hızı (m/s). GNSS sinyallerinin uydu ile alıcı arasında hareket etme hızını temsil edecek.
RE_WGS84 = 6_378_137   # Dünya Yarıçapı (WGS84 yöntemiyle ölçülmüştür) (m). Dünya'nın WGS84 referans elipsoidine göre ekvator yarıçapını temsil edecek.
OMGE = 7.2921151467E-5  # Dünya'nın Açısal Hızı (rad/s). Dünya'nın kendi ekseni etrafındaki dönüş hızını temsil eder.  Konum hesaplamalarında kullanılır.

"""# 3- Uydu Seçimi

Bu fonksiyon, bir GNSS veri çerçevesinden (ör. device_gnss.csv dosyasından alınan veriler) belirli kriterlere göre uydu sinyallerini seçmek için kullanılacaktır.

Fonksiyon, aşağıdaki ölçütlere uymayan uydu sinyallerini filtreleyerek daha güvenilir veriler elde edilmesini sağlar:
- **Taşıyıcı frekans hatası (CarrierErrorHz) 2 MHz'ten küçük olmalı.** Çünkü, GNSS sinyallerinde taşıyıcı frekans hatasının küçük olması, sinyalin doğru bir şekilde alınması için önemlidir. 2 MHz üzerindeki hatalar genellikle sinyaldeki parazit veya sapmalardan kaynaklanır ve güvenilir veri elde edilmesini zorlaştırır.

- **Uydu eğim açısı (SvElevationDegrees) 10 dereceden büyük olmalı.** Çünkü, 10 derece altındaki açılar veya 15 derece üstündeki açılar, sinyalin atmosferik etkilerden (örneğin troposferik kırılma ya da bir diğer deyişle GPS uydusunun sinyalinin varışındaki gecikme) daha fazla etkilenmesine neden olabilir. Ayrıca, alıcıya gelen sinyalin zayıf olması düşük eğim açılarında daha olasıdır.

- **Taşıyıcı-gürültü oranı (Cn0DbHz) 15 dB-Hz'den büyük olmalı.** Çünkü, Taşıyıcı-gürültü oranı, sinyal kalitesini gösteren önemli bir metriktir. 15 dB-Hz altındaki sinyaller zayıf olarak kabul edilir ve konum belirleme doğruluğunu olumsuz etkileyebilir.

- **Çoklu yol etkisi (MultipathIndicator) olmadığını belirtmeli.** Çünkü, Çoklu yol etkisi, GNSS sinyallerinin yüzeylerden yansıması sonucu farklı yollardan alıcıya ulaşmasıdır. Bu durum, sinyalin doğruluğunu ciddi şekilde bozabilir. Bu nedenle, yalnızca doğrudan gelen sinyaller seçilmelidir.

Bu fonksiyonu yazmamızdaki sebep, GNSS sinyallerinin doğruluğunu artırmak ve hatalı sinyallerden kaynaklanan sapmaları azaltmaktır.
"""

# GNSS veri çerçevesinden uydu sinyallerini belirli kriterlere göre filtreleyen fonksiyon
def satellite_selection(df, column):
    """
    Bu fonksiyon, GNSS veri çerçevesinden belirli kriterlere uymayan uydu sinyallerini filtreler.

    Args:
        df : DataFrame from device_gnss.csv
            GNSS ölçümlerini içeren veri çerçevesi.
        column : str
            Sinyallerin filtreleneceği sütunun adı.

    Returns:
        df : DataFrame
            Belirtilen kriterlere uyan uydu sinyallerini içeren filtrelenmiş veri çerçevesi geri döndürülecek.
    """
    # Null olmayan değerlerle çalışmak için
    idx = df[column].notnull()

    # Taşıyıcı frekans hatası (CarrierErrorHz) 2 MHz'ten küçük olmalı. Çünkü:
    #  - GNSS sinyallerinde taşıyıcı frekans hatasının küçük olması, sinyali doğru bir şekilde almamız için önemlidir.
    #  - 2 MHz üzerindeki hatalar genellikle sinyaldeki parazit veya sapmalardan kaynaklanır ve güvenilir veriyi elde etmemizi zorlaştırır.
    idx &= df['CarrierErrorHz'] < 2.0e6

    # Uydu eğim açısı (SvElevationDegrees) 10 ila 15 derece arasında olmalı. Çünkü:
    #  - 10 derece altındaki açılar, sinyalin atmosferik etkilerden (örneğin troposferik kırılma - sinyaldeki gecikme) daha fazla etkilenmesine neden olabilir.
    #  - Ayrıca,  eğim açısı düştükçe alıcıya gelen sinyalin zayıflaması daha olasıdır.
    idx &= df['SvElevationDegrees'] > 10.0

    # Taşıyıcı-gürültü oranı (C/N0) 15 dB-Hz'den büyük olmalı. Çünkü:
    # - Taşıyıcı-gürültü oranı, sinyal kalitesini gösteren önemli bir metriktir.
    # - 15 dB-Hz altındaki sinyaller zayıf olarak kabul edilir ve konum belirleme doğruluğunu olumsuz etkileyebilir.
    idx &= df['Cn0DbHz'] > 15.0

    # Çoklu yol etkisi (MultipathIndicator = 0) olmamalı. Çünkü:
    # - Çoklu yol etkisi, GNSS sinyallerinin yüzeylerden yansıması sonucu farklı yollardan alıcıya ulaşmasıdır.
    # - Bu durum, sinyalin doğruluğunu ciddi şekilde bozabilir. Bu nedenle, yalnızca doğrudan gelen sinyaller seçilmeli , yüzeyden seken sinyalleri egale etmeliyiz.
    idx &= df['MultipathIndicator'] == 0

    # Belirtilen kriterlere uyan satırları döndür
    return df[idx]

"""# 4- Uydu Sinyallerinden Yeryüzündeki Bir Kullanıcının Konumunu ve Hızını Hesaplama (Weighted Least Squares / Ağırlıklı En Küçük Kareler Yönemi İle)

GNSS sistemlerinde alıcıya gelen sinyallerin içerdiği veriler (örneğin pseudorange) her zaman aynı kalitede değildir. Bazı uydulardan gelen sinyaller gürültülüdür, bazıları daha güvenilirdir.

WLS, her ölçüme eşit ağırlık vermek yerine, güvenilirliği yüksek olan ölçümlere daha fazla, düşük güvenilirliğe sahip olanlara ise daha az ağırlık vererek tahmin yapar.

Bu Projedeki Rolü
Projedeki işlem sırasına göre:

  - Her epoch (zaman adımı) için GNSS uydularından gelen veriler alınır.

  - Bu verilerin içinde RawPseudorangeUncertaintyMeters gibi belirsizlik ölçümleri yer alır.

  - WLS, bu belirsizlik değerlerini ağırlık matrisi olarak kullanır ve konum tahmini yapar.



Aynı mantıkla, hız tahmini için de WLS kullanılır (PseudorangeRateUncertaintyMetersPerSecond).

Bu sayede:

  - Gürültülü uydu sinyallerinin etkisi azaltılır.

  - Konum ve hız tahmini daha hassas olur.

  - Kalman filtresi veya LSTM gibi modellerden önce güvenilir başlangıç değerleri elde edilir.

Sonuç olarak bu projede WLS, GNSS verileri ile:

  - İlk konum ve hız tahminlerini sağlamak,

  - Gürültülü verilerin etkisini azaltmak,

  - Sonraki adımlarda kullanılacak (örneğin Kalman veya LSTM gibi) modellere sağlam zemin hazırlamak için kritik bir rol oynar.


Özetle:
  -  WLS, "ham veriden sağlam tahmin" için başlangıç filtresidir.
  - Kalman, "akıllı düzeltici", LSTM ise "veriden öğrenen öngörücü" olarak işin devamını getirir.
"""

def point_positioning(gnss_df):
    # Her uydu (Svid) ve sinyal tipi (SignalType) için taşıyıcı frekansı (CarrierFrequencyHz) sütunundaki medyan değerleri hesaplıyoruz.
    CarrierFrequencyHzRef = gnss_df.groupby(['Svid', 'SignalType'])['CarrierFrequencyHz'].median()
    # Yukarıda oluştruduğumuz her medyan değerini, orijinal GNSS verileriyle birleştirdik
    gnss_df = gnss_df.merge(CarrierFrequencyHzRef, how='left', on=['Svid', 'SignalType'], suffixes=('', 'Ref'))

    # Üstte Hesapladığım CarrierFrequencyHzRef ile veri setinde bulunan CarrierFrequencyHz arasındaki farkı bularak taşıyıcı frekansı hatasını (CarrierErrorHz) hesapladık ...
    # ... ve veri setimizde 'CarrierErrorHz' sütununu ekledik
    # Çünkü başlarda da bahsettiğimiz gibi Taşıyıcı frekans hatası (CarrierErrorHz) 2 MHz'ten küçük olursa işimize yarayacak
    gnss_df['CarrierErrorHz'] = np.abs((gnss_df['CarrierFrequencyHz'] - gnss_df['CarrierFrequencyHzRef']))

    # Pseudorange verisini (gerçek mesafe ölçümünden elde edilen değer) Carrier Smoothing (taşıyıcı-pürüzsüzleştirme) yöntemiyle düzgünleştirdik.
    # Basitçe Carrier Smoothing: pseudorange verisinin doğruluğunu artırmak için kullanılan bir tekniktir. Verilerdeki gürültüyü azaltır ve daha doğru yer belirleme (positioning) sağlar
    gnss_df = carrier_smoothing(gnss_df)

    # Benzersiz zaman damgalarını alacak ve toplam iterasyon sayısını hesaplayacağız. Yani, kaç farklı zaman noktasının (epoch) veri setinde bulunduğunu göstereceğiz
    utcTimeMillis = gnss_df['utcTimeMillis'].unique() # "utcTimeMillis", her bir veri kaydının zamanını milisaniye cinsinden temsil ediyor.
    nepoch = len(utcTimeMillis)  # Toplam iterasyon sayısı

    # Pozisyon ve hız için başlangıç vektörlerini tanımladık
    x0 = np.zeros(4)  # Başlangıç pozisyon [x, y, z, saat] -> her birine ilk değer olarak 0 atadık
    v0 = np.zeros(4)  # Başlangıç hız [vx, vy, vz, saat] -> her birine ilk değer olarak 0 atadık

    # Pozisyon ve hız tahminlerini kayıt etmek için matrisler oluşturacağız
    x_wls = np.full([nepoch, 3], np.nan)  # Pozisyon tahminlerini kaydetmek için
    v_wls = np.full([nepoch, 3], np.nan)  # Hız tahminlerini kaydetmek için


    for i, (t_utc, df) in enumerate(tqdm(gnss_df.groupby('utcTimeMillis'), total=nepoch)):
        # Geçerli uydu sinyallerini Pseudorange verisi için seçelim
        df_pr = satellite_selection(df, 'pr_smooth')
        # Geçerli uydu sinyallerini Pseudorange Rate verisi için seçelim
        df_prr = satellite_selection(df, 'PseudorangeRateMetersPerSecond')  # PseudorangeRateMetersPerSecond, pseudorange'nin zamanla değişme hızını (yani, alıcı ile uydu arasındaki mesafenin hızını) ifade eder.

        # GNSS verisindeki pseudorange hatalarını düzeltmek için gerekli düzenlemeler yapacağız:
        # - SvClockBiasMeters: Uydu saatinin yanlışı (saat kayması)
        # - IsrbMeters:  uydunun yerçekimi, ortam koşulları veya diğer etmenler nedeniyle oluşan küçük sapmaları ifade eder
        # - IonosphericDelayMeters (İyonosferik Gecikme): GNSS sinyalleri, iyonosferdeki serbest elektron yoğunluğu nedeniyle hız kaybına uğrar.
        # - TroposphericDelayMeters (Troposferik Gecikme): Troposferdeki nem, sıcaklık ve basınç faktörler,, GNSS sinyallerini etkileyebilir ve bu da sinyallerin alıcıya ulaşma süresini uzatır.
        pr = (df_pr['pr_smooth'] + df_pr['SvClockBiasMeters'] - df_pr['IsrbMeters'] - df_pr['IonosphericDelayMeters'] - df_pr['TroposphericDelayMeters']).to_numpy()

        # Pseudorange Rate (Pseudorange Hızı) ve SvClock Drift (Uydu Saat Kayması) verilerini birleştirilerek, düzeltilmiş Pseudorange Rate elde edeceğiz.
        prr = (df_prr['PseudorangeRateMetersPerSecond'] + df_prr['SvClockDriftMetersPerSecond']).to_numpy()

        # Uydu pozisyon verilerini Pseudorange verisi için alacağız:
        #  - ECEF (Earth-Centered, Earth-Fixed): Dünya merkezine dayalı bir koordinat sistemidir.
        xsat_pr = df_pr[['SvPositionXEcefMeters', 'SvPositionYEcefMeters','SvPositionZEcefMeters']].to_numpy()

        # Uydu pozisyon verilerini Pseudorange Rate verisi için hız için alacağız
        xsat_prr = df_prr[['SvPositionXEcefMeters', 'SvPositionYEcefMeters','SvPositionZEcefMeters']].to_numpy()

        # Uydu hız verilerini Pseudorange Rate verisi için alacağız
        vsat = df_prr[['SvVelocityXEcefMetersPerSecond', 'SvVelocityYEcefMetersPerSecond','SvVelocityZEcefMetersPerSecond']].to_numpy()

        # Pseudorange ve Pseudorange Rate için ağırlık matrislerini hesaplayacağız
        # Buradaki "1 / df_pr['RawPseudorangeUncertaintyMeters']" işlemi:
        # Her bir belirsizliğin tersini alır. Belirsizliğin tersten alınması, daha düşük belirsizliğe sahip ölçümlere daha fazla ağırlık verir...
        # ... Yüksek belirsizlik, düşük ağırlık anlamına gelir. Yani, güvenilir olmayan ölçümler daha düşük bir ağırlığa sahip olmalı.
        Wx = np.diag(1 / df_pr['RawPseudorangeUncertaintyMeters'].to_numpy())
        Wv = np.diag(1 / df_prr['PseudorangeRateUncertaintyMetersPerSecond'].to_numpy())


        # Artık kullanıcının POZİSYON TAHMİNİNİ yapabiliriz

        if len(df_pr) >= 4:  # Konumu doğru belirleyebilmek için en az 4 uydu gerekir -> 3D konum + zaman
            if np.all(x0 == 0):  # Başlangıç tahmini 0 ise
                # Bu kısımda, LEAST SQUARES (en küçük kareler) yöntemi kullanılarak pozisyon tahmini yapacağız:
                # - pr_residuals: Bu fonksiyon, modelle uydu ölçümleri arasındaki farkları hesaplayacak. (residuals = artık)
                # - jac_pr_residuals: ölçüm hatalarının (residul) türevini alarak doğrusal olmayan optimizasyonun yapılmasını sağlar.
                # - xsat_pr: Uyduların konum verilerini içeren NumPy dizisi.
                # - pr: Pseudorange (mesafe ölçüm) verileri.
                # - Wx: Pseudorange belirsizliklerine dayalı ağırlık matrisi.
                opt = scipy.optimize.least_squares(pr_residuals, x0, jac_pr_residuals, args=(xsat_pr, pr, Wx))

                # Daha sonra yukarıda optimize edilen pozisyonu güncel pozisyonumuz olarak güncelledik
                x0 = opt.x

            # Normal WLS'ten sonra Robust (dayanıklı) WLS ile pozisyon tahmini yaparak ölçüm doğruluğunu arttıracağız:
            # Burada" soft_l1" loss fonksiyonu kullanılarak, pozisyon tahmini yapılırken daha az güvenilir ölçümlerin etkisi azaltılmak istiyoruz.
            opt = scipy.optimize.least_squares(pr_residuals, x0, jac_pr_residuals, args=(xsat_pr, pr, Wx), loss='soft_l1')


            # Optimizasyonun durumunu kontrol ederek başarılı mı başarısız mı diye bakalım
            # - (0) = Başarılı optimizasyon
            # - (1) = Başarıyla tamamlanmış ancak istenen hassasiyete ulaşılmamış.
            # - (2) = Optimizasyon başarıyla tamamlanmış ancak maksimum iterasyon sayısına ulaşılmış. Yani, çözüme ulaşmak için daha fazla iterasyon yapılabilirdi ancak durdurulmuş.
            # - (-1): Başarısızlık, optimizasyonun çözüme ulaşamadığı durum.
            # - (-2): Başarısızlık.
            if opt.status < 1 or opt.status == 2:
                print(f'i = {i} KONUM optimizasyon durumu = {opt.status}')

            else:
              # Eğer optimizasyon başarılı bir şekilde tamamlanmışsa, tahmin edilen pozisyon (opt.x) alınır ve x_wls'ye kaydedilir.
              # Buradaki opt.x[:3] kısmı, elde edilen 4 elemanlı çözüm vektörünün (3D + Zaman) ilk 3 elemanını (x, y, z koordinatlarını) alır.
                x_wls[i, :] = opt.x[:3]  # Tahmin edilen pozisyonu kaydet
                x0 = opt.x  # Pozisyonu güncelle




        # Kullanıcının HIZ TAHMİNİNİ de yapabliriz

        # Üstte konum için yaptığımız tüm işlemlerin aynısını hız için de yapacağız
        if len(df_prr) >= 4:  # En az 4 uydu verisi gerekli demiştik
            if np.all(v0 == 0):  # Başlangıç tahmini yoksa normal WLS ile başla
                opt = scipy.optimize.least_squares(prr_residuals, v0, jac_prr_residuals, args=(vsat, prr, x0, xsat_prr, Wv))
                v0 = opt.x  # Optimize edilen hızı güncelledik
            # Robust (Dayanıklı) WLS ile hız tahmini yaptık
            opt = scipy.optimize.least_squares(prr_residuals, v0, jac_prr_residuals, args=(vsat, prr, x0, xsat_prr, Wv), loss='soft_l1')
            if opt.status < 1:  # Yİne optimizaston durumlarını kontrol edeceğiz
                print(f'i = {i} HIZ optimizasyon durumu = {opt.status}')
            else:
                v_wls[i, :] = opt.x[:3]  # Tahmin edilen hızı kaydet
                v0 = opt.x  # Hızı güncelle

    # Tahmin edilen zaman, pozisyon ve hız değerlerini döndür
    return utcTimeMillis, x_wls, v_wls

"""# 5- Carrier Smoothing (Taşıyıcı Pürüzsüzleştirme) ile Pseudorange (Gerçek Mesafa , Konum/Hız Tahmini) Tahmininin Doğruluğunu Artırma

Bu fonksiyon, GNSS verilerinde taşıyıcı-faz ve pseudorange ölçümlerini birleştirerek taşıyıcı pürüzsüzleştirme (carrier smoothing) yöntemiyle daha hassas bir pseudorange tahmini yapar. Bu işlem, sinyal hatalarını ve sapmaları azaltmayı hedefler.

Pseudorange tahmini, GNSS (Global Navigation Satellite System) verilerinden kullanıcı konumunu ve hızını belirlemek için temel bir girdidir. Bu tahminler, uydu ile alıcı arasındaki ölçülen mesafeyi temsil eder, ancak bu mesafeler çeşitli hatalardan (örneğin, iyonosferik ve troposferik gecikmeler, saat hataları) etkilenir. **Carrier smoothing, bu hataları azaltarak daha güvenilir bir pseudorange tahmini sağlar.**
"""

def carrier_smoothing(gnss_df):
    """
    Returns:
        df:'pr_smooth' adında taşıyıcı pürüzsüzleştirilmiş pseudorange DataFram'i

    """

    # Eşik Değerlerimizi tanımlayalım:

      # Taşıyıcı faz sıçrama eşiği [metre].
      # Bu değer, taşıyıcı faz ölçümlerindeki ani sıçramaların (cycle slip) tolerans sınırını belirler.
    carr_th = 1.6

      # Pseudorange sıçrama eşiği [metre].
      # Bu değer, pseudorange ölçümlerinde ani değişimlerin hata olarak algılanması için sınır belirler.
    pr_th =  20.0



    # pseudorange değerlerini taşıyıcı-pürüzsüzleştirme (carrier smoothing) işlemi sonrası saklayacak bir dizi oluşturduk
    prsmooth = np.full_like(gnss_df['RawPseudorangeMeters'], np.nan)  # np.nan ile birlikte pürüzsüzleştirme işlemi sonrası değerleri yer değiştireceğiz


    # Her bir sinyal için taşıyıcı-pürüzsüzleştirme (carrier smoothing) işlemini gerçekleştireceğiz:
    for (i, (svid_sigtype, df)) in enumerate((gnss_df.groupby(['Svid', 'SignalType']))):

        # 'AccumulatedDeltaRangeMeters' (Birikmiş Delta Menzil Ölçerler) sütunundaki 0 değerlerini NaN (geçersiz) ile değiştiriyoruz...
        # ... Böylece hatalı verilerin işlemi etkilemesini engelleyeceğiz.
        df = df.replace({'AccumulatedDeltaRangeMeters': {0: np.nan}})


        # Doppler hız bilgisiyle pseudorange (drng2) ve taşıyıcı faz (drng1) değişimlerini karşılaştıracağız...
        # ... Böylece, döngü kaymaları (cycle-slip) gibi hataları tespit edeceğiz.
        drng1 = df['AccumulatedDeltaRangeMeters'].diff() - df['PseudorangeRateMetersPerSecond']
        drng2 = df['RawPseudorangeMeters'].diff() - df['PseudorangeRateMetersPerSecond']


        # Farklı durumlara göre döngü kayması (cycle-slip) hatalarını kontrol etmeliyiz. Yani 'hata türlerini' belirlemeliyiz:
          # - slip3: Taşıyıcı faz değişimi belirlediğimiz eşik değerini (carr_th) aşarsa hata olarak işaretlenecek.
          # - slip4: Pseudorange değişimi belirlediğimiz eşik değerini (pr_th) aşarsa hata olarak işaretlenecek.
        slip1 = (df['AccumulatedDeltaRangeState'].to_numpy() & 2**1) != 0  # sıfırlama bayrağı
        slip2 = (df['AccumulatedDeltaRangeState'].to_numpy() & 2**2) != 0  # döngü kayma bayrağı
        slip3 = np.fabs(drng1.to_numpy()) > carr_th
        slip4 = np.fabs(drng2.to_numpy()) > pr_th


        # Üstte oluşturduğumuz döngü hata türlerini bulduğumuzda bunları işaretleyeceğiz:
        idx_slip = slip1 | slip2 | slip3 | slip4 # Herhangi bir döngü kayması durumu 'True' olarak işaretlenecek. Bunun için üstte belirlediğimiz hata türlerini bir değişkende tuttuk.
        # İlk veri noktasını True olarak işaretleyeceğiz. Çünkü ilk veri noktasından önce bir veri olmadığı için döngü kayması içeriyor gini değerlendirdik.
        idx_slip[0] = True



        # Sürekli taşıyıcı faz izleme gruplarını oluştur
        df['group_slip'] = np.cumsum(idx_slip)  # Her döngü kayması tespit edildiğinde grup numarası bir artırılır.




        # Pseudorange (ham mesafe ölçümü) ile taşıyıcı faz ölçümü arasındaki farkı hesapladık
        # Bu fark [dpc], ölçüm hatalarının analizinde ve taşıyıcı faz ölçümlerine dayalı doğruluğun artırılmasında önemli bir rol oynayacak
        df['dpc'] = df['RawPseudorangeMeters'] - df['AccumulatedDeltaRangeMeters']




        # Üstte oluşturduğumuz group_slip'e göre her gruptaki dpc(Psudorange - carrier phase) değerlerinin  ortalamasını hesapladık
        meandpc = df.groupby('group_slip')['dpc'].mean()
        df = df.merge(meandpc, on='group_slip', suffixes=('', '_Mean')) # Bu ortalamaları df'e ekledik



        # Hata içeren uydu id'lerini ve sinyal tiplerini orijinal gnns_df'te buluyoruz
        idx = (gnss_df['Svid'] == svid_sigtype[0]) & (gnss_df['SignalType'] == svid_sigtype[1])



        # Bu gata içeeren uydulara taşıyıcı faz ölçümü ve bias'ı ekleyerek düzgünleştirilmiş pseudorange'ı hesaplıyoruz
        prsmooth[idx] = df['AccumulatedDeltaRangeMeters'] + df['dpc_Mean']



    # Eğer taşıyıcı-pürüzsüzleştirme mümkün değilse, orijinal pseudorange değerini kullanacağız:
    idx_nan = np.isnan(prsmooth) # NaN değerlerini bul
    prsmooth[idx_nan] = gnss_df['RawPseudorangeMeters'][idx_nan]  # NaN olan yerlerde orijinal pseudorange'ı kullan
    gnss_df['pr_smooth'] = prsmooth # Sonuçları 'pr_smooth' sütununa ata

    return gnss_df

"""# 6- Pozisyon Tahmin ve Artıklık Hesaplamaları (Psödorange/Doppler Kalıntıları ve Jacobian Matrisi)
Bu kısımda yazacağımız 5 fonksiyon kullanıcı ve uydu arasındaki pozisyon, hız ve diğer etkenleri dikkate alarak GNSS verilerinden doğruluğu artıran hesaplamalar yapacaktır.

Residuals = Kalıntı , Artık , Sapma
<br></br>

**Sagnac Etkisi Nedir?**

Sagnac Etkisi, dünyanın dönmesi nedeniyle ışığın hızında küçük bir fark oluşmasıdır. Bunu anlamak için basit bir örnek düşünelim:

Bir dönen atlı karıncaya bindik ve elimizde bir el feneri var. Eğer feneri atlı karıncanın dönüş yönünde tutarsak, ışığın hedefe ulaşması biraz daha uzun sürebilir. Ama ters yönde tutarsak, ışık daha hızlı ulaşır. İşte bu küçük zaman farkı Sagnac Etkisidir.

Bu etki, hassas ölçümler yapan cihazlarda fark edilir ve özellikle GPS sistemleri ve jiroskoplarda dikkate alınır. Eğer düzeltilmezse, navigasyon sistemleri birkaç metre hata yapabilir. Bu yüzden mühendisler, Dünya'nın dönüşünden kaynaklanan bu etkiyi hesaba katarak sistemleri tasarlar.
"""

# los_vector(xusr, xsat) fonksiyonu, kullanıcı (xusr) ile uydu (xsat) arasındaki doğrultu vektörünü (line-of-sight vector) ve mesafeyi hesaplayacak.
# GNSS konumlandırma sistemlerinde temel bir fonksiyondur çünkü doğru bir pozisyon tahmini için doğrultu ve mesafe bilgisi gereklidir.
def los_vector(xusr, xsat):
    """
    Args:
        xusr : ECEF'teki kullanıcı konumu (m) (ECEF = Earth Centered Earth Fixed Koordinat Sistemi)
        xsat : ECEF'teki uydu konumu (m)
    Returns:
        u: ECEF'teki kullanıcıdan uyduya doğru olan birim doğrultu vektörü (unit line-of-sight)  (m)
        rng: Kullanıcı ve uydu arasındaki mesafe (m)
    """
    u = xsat - xusr # doğrultunun yönü
    rng = np.linalg.norm(u, axis=1).reshape(-1, 1)  # Kullanıcı ve uydu arasındaki mesafe
    u /= rng

    return u, rng.reshape(-1)





# Jacobian matrisini hesaplayan fonksiyonumuz
# Bu matris, konum tahmin hatalarını minimize etmek için kullanılan bir optimizasyon yöntemi.
def jac_pr_residuals(x, xsat, pr, W):
    """
    Args:
        x : ECEF'teki mevcut pozisyon (m)
        xsat : ECEF'te uydu pozisyonu (m)
        pr : pseudorange (m)
        W : ağırlık matrisi
    Returns:
        W*J : Jacobian matrisi
    """
    u, _ = los_vector(x[:3], xsat) # Kullanıcı ve uydu arasındaki doğrultu vektörünü hesaplayacak. Buradaki 'u, _' işlemi =  (paketten çıkarma) özelliğidir. Fonksiyondan dönen birden fazla değeri birden fazla değişkene atamak için kullanılır.
                                   # Kullanıcının ilk 3 koordinatını alacak
    J = np.hstack([-u, np.ones([len(pr), 1])])   # Jacobian matrisi: [-ux -uy -uz 1]

    return W @ J  # Ağırlıklı Jacobian matrisi, yani W ile çarpılmış J matrisini bize döndürecek






# Pseudorange artıklarını hesaplayan fonksiyonumuz.
# Yani, kullanıcı pozisyonu ile uydu arasındaki mesafe farkı ile ölçülen pseudorange (gerçek mesafe) arasındaki fark
def pr_residuals(x, xsat, pr, W):
    """
    Args:
        x : kullanıcının ECEF'teki mevcut pozisyon (m)
        xsat : ECEF'te uydu pozisyonu (m)
        pr : pseudorange (m)
        W : ağırlık matrisi
    Returns:
        residuals*W : pseudorange hataları , artıkları
    """
    u, rng = los_vector(x[:3], xsat)  # Üstte yazdığımıuz los_vector fonksiyonu çağırarak  kullanıcı ile uydu arasındaki doğrultu vektörünü (u) ve mesafeyi (rng) hesaplıyoruz.

    # ! Mesafeyi hesaplarken Dünya'nın dönüşünden kaynaklanan 'Sagnac Etkisi'ni dikkate almalıyız.
    # Burada, OMGE Dünya'nın açısal hızı (radyan/saniye) ve CLIGHT ışık hızı (metre/saniye) sabitlerini kullanılarak bir düzeltme uyguladık.
    rng += OMGE * (xsat[:, 0] * x[1] - xsat[:, 1] * x[0]) / CLIGHT

    # Add GPS L1 clock offset
    residuals = rng - (pr - x[3]) # Gerçek mesafe ile ölçülen pseudorange arasındaki fark yani artık
                                  # 'rng': Hesaplanan gerçek mesafe , 'pr - x[3]': Ölçülen pseudorange değeri.
                                  # x[3] = zaman bileşenini temsil ediyor (x,y,z ve zaman'dan zaman'ı aldık)

    return residuals @ W   # Artıkları ağırlık matrisi ile çarpıp bize geri döndürecek





# Bu fonksiyon,  kullanıcının ve uydunun hızları ve pozisyonlarına dayanarak pseudorange rate (PRR)'i için Jacobian matrisini hesaplayacak.
# Bu da pozisyon ve hız tahmininin doğruluğunu optimize etmekte kullanılacak
def jac_prr_residuals(v, vsat, prr, x, xsat, W):
    """
    Args:
        v : Kullanıcının mevcut hızı, ECEF koordinat sisteminde (m/s).
        vsat : Uydu hızı, ECEF koordinat sisteminde (m/s).
        prr : Pseudorange rate (m/s), uydu ile kullanıcı arasındaki mesafe değişim hızıdır.
        x : Kullanıcının mevcut pozisyonu, ECEF koordinat sisteminde (m).
        xsat : Uydunun pozisyonu, ECEF koordinat sisteminde (m).
        W : Ağırlık matrisi
    Returns:
        W*J : Jacobian matrisi
    """
    u, _ = los_vector(x[:3], xsat) # 2. fonksitonda da yaptığımız kullanıcı ve uydu arasındaki doğrultu vektörünü hesaplayan işlem.
    J = np.hstack([-u, np.ones([len(prr), 1])])  # Jacobian matrisi [-ux -uy -uz 1] şeklinde olmalı

    return np.dot(W, J)  # W matrisi ile J matrisini çarpımını bize döndür






# Pseudorange rate artıklarını hesaplyacağız
def prr_residuals(v, vsat, prr, x, xsat, W):
    """
    Args:
        v : Kullanıcının mevcut hızı, ECEF koordinat sisteminde (m/s).
        vsat : Uydu hızı, ECEF koordinat sisteminde (m/s).
        prr : Pseudorange rate (m/s), uydu ile kullanıcı arasındaki mesafe değişim hızıdır.
        x : Kullanıcının mevcut pozisyonu, ECEF koordinat sisteminde (m).
        xsat : Uydunun pozisyonu, ECEF koordinat sisteminde (m).
        W : Ağırlık matrisi
    Returns:
        residuals*W : pseudorange rate artıkları , hataları
    """
    u, rng = los_vector(x[:3], xsat)

    # Pseudorange rate hesaplaması yapacağız:
      # İlk olarak uydu hızından kullanıcı hızını çıkararak doğrultu vektörü (u) ile çarpıp toplamını alıyoruz
      # Burda da yine aynı şekilde Dünya dönmesinin etkisini 'Sagnac Etkisi'ni göz önünde bulunduruyoruz
    rate = np.sum((vsat - v[:3]) * u, axis=1) + OMGE / CLIGHT * (vsat[:, 1] * x[0] + xsat[:, 1] * v[0] - vsat[:, 0] * x[1] - xsat[:, 0] * v[1])

    # Rate'i bulduğumuza göre artıkları (prr ile tahmin edilen hız arasındaki fark) hesaplayabiliriz
    residuals = rate - (prr - v[3])

    return residuals @ W

"""# 7-  Aracın Tümsekten Geçerken Zıplması Dolayısıyla Oluşturduğu Aykırı Değer  ve İnterpolasyonu (Veri Düzeltme)

Bu fonksiyon, aykırı değer tespiti yaparak, özellikle z ekseni üzerindeki (yukarı) hız bileşeninde belirlediğimiz eşik değerini aşan anormal hızları (örneğin, gerçekte bir araç yolda yatay olarak hareket ederken, GNSS verisi aracın aniden yukarı doğru (havaya) fırladığı gibi bir hız değeri rapor edebilir.) belirler ve bu değerleri geçici olarak geçersiz (NaN) olarak işaretler. Daha sonra, eksik verileri interpolasyon yöntemleriyle doldurarak hem pozisyon hem de hız verilerindeki hataları düzeltmemizi sağlar. Bu fonksiyon sayesinde GNSS verilerinin daha doğru ve tutarlı olmasını amaçlıyoruz:

- Aykırı değer tespitinde: Z eksenindeki hız bileşeni için belirli bir eşiği aşan değerler NaN olarak işaretleyeceğiz.
- Interpolasyon: Eksik (NaN) değerleri lineer veya spline interpolasyon yöntemleriyle dolduracağız.
(Komşu veri noktalarına göre bir doldurma işlemi yapılır):
![image.png](data:image/png;base64,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)

- y1 ve y2 -> Eksik verinin iki komşu noktası
- x1 ve x2 -> Bu noktaların indeksleri (zaman veya pozisyon bilgisi).
- x -> eksik verinin indeksi

Koordinat Sistemleri ve	Kullanım Alanları:

- **ECEF**	Uydular arası mesafe, küresel analizler, uydu yörüngelerinin modellenmesi.
- **Jeodezik**, arazi ölçümü, kullanıcı odaklı konum gösterimi.
- **ENU**,	Yerel düzlemde hareket analizi (örneğin, bir aracın kuzeye doğru hızı), yerel navigasyon
"""

# Simple outlier detection and interpolation
def exclude_interpolate_outlier(x_wls, v_wls):
    """
    Args:
        x_wls : Konum vektörü
        v_wls : Hız vektörü
    """

    # z ekseninde hız için eşik değerimizi tanımlayalım (m/s)
    v_up_th = 2.0


    # Konum ve hız bilgilerini ECEF'ten farklı bir koordinat sistemine dönüştüreceğiz:
      # Konumu, ECEF'ten Jeodezik koordinata çevirdik. Jeodezik kkoordinatlar, dünya yüzeyi üzerindeki noktaları daha kolay temsil etmemizi sağlayacak
      # Hızı, ECEF'ten ENU koordinat sistemine çevirdik. ENU(Earth North Up), aracın hareket yönünü belirlememizi ve analiz etmemizi kolaylaştırır.
    x_llh = np.array(pm.ecef2geodetic(x_wls[:, 0], x_wls[:, 1], x_wls[:, 2])).T
    v_enu = np.array(pm.ecef2enuv(v_wls[:, 0], v_wls[:, 1], v_wls[:, 2], x_llh[0, 0], x_llh[0, 1])).T


    # Hız vektörü belirlediğiiz eşik değerin üstündyse aykırı değer ataması yapıp uerine NaN değeri atıyoruz
    # Burada aslında bahsettiğimiz şey arabaların tümseklerden geçerkenki yukarı doğru olan hız vektörü
    idx_v_out = np.abs(v_enu[:, 2]) > v_up_th
    v_wls[idx_v_out, :] = np.nan


    # Konum verilerindeki eksik değerleri interpolasyon yöntemi ile dolduracağız
    x_df = pd.DataFrame({'x': x_wls[:, 0], 'y': x_wls[:, 1], 'z': x_wls[:, 2]}) # konum verilerini df'e dönüştürdük
    x_df = x_df.interpolate(limit_area='outside', limit_direction='both') # interpolasyon ile eksik verileri doldurduk (komşu veri noktalarına göre)


    # Aynı şekilde Hız verilerindeki eksik değerleri interpolasyon yöntemi ile dolduracağız
    v_df = pd.DataFrame({'x': v_wls[:, 0], 'y': v_wls[:, 1], 'z': v_wls[:, 2]})
    v_df = v_df.interpolate(limit_area='outside', limit_direction='both')
    v_df = v_df.interpolate('spline', order=3) # kübik spline yöntemi kullandık. Bu sayede daha akıcı bir tahmin elde edebileceğiz


    return x_df.to_numpy(), v_df.to_numpy()

"""# 8- Kalman Filtreleme ve Pürüzsüzleştirme

Kalman filtresi, dinamik sistemlerde en uygun durum tahminini sağlayan ve zaman serisi verilerinin analizi için kullanılan bir algoritmadır. Bu filtre, önceki durum bilgilerine dayanarak tahmin yapar ve yeni ölçümleri entegre ederek sistemin mevcut durumunu optimize eder.

İlk olarak, başlangıç durumu belirlenir ve tahmin yapılır. Ardından, bir ölçüm elde edilir ve model ile ölçüm entegrasyonu Kalman kazancı ile yapılır; kazanç, model ve ölçüm belirsizliklerine göre ayarlanır. Bu tekrar eden tahmin-güncelleme süreci, daha doğru tahminler yapılmasını sağlar

GNSS konum iyileştirme sistemlerinde, ölçüm gürültülerini ve hataları azaltmak için yaygın olarak kullanılmaktadır. GNSS verilerinin Kalman filtresiyle işlenmesinde durum ve gözlem bilgilerini entegrasyon yoluyla daha hassas bir konum tahmini yapılabilir. Kalman filtresi, sistemdeki gürültü ve belirsizlikleri en aza indirgeyerek en olası durumu belirlemeyi amaçlar.

Kalman Filtresi için basitçe "geçmişten ders alarak geleceği tahmin eder" diyebiliriz.

> Kalman Filtresi de önceki verileri kullanarak tahmin yapıyor, LSTM de. O zaman bu projede neden LSTM’i de kullandın?



Kalman Filtresi, sistemin davranışını önceden tanımlanmış fiziksel ve matematiksel modellere göre tahmin eder. Örneğin, GNSS sinyallerinde konum, hız ve ivme gibi parametrelerin zaman içinde nasıl değiştiği, belirli hareket denklemleriyle ifade edilir ve bu modele uygun olarak tahmin yapılır. Ancak bu model belirli kabuller içerir (örneğin: Gaussian gürültü, lineer hareket gibi) ve karmaşık, doğrusal olmayan sistemlerde ya da veri kalitesi düşük olduğunda sınırlı performans gösterir.

LSTM ise, geçmiş verilerden temsilî kalıpları öğrenerek bu sınırlamaları aşabilir. Özellikle GNSS sinyalleri gibi zaman serisi içinde trend, örüntü ve bozulmaların modellenmesi gerektiği durumlarda, LSTM geçmiş verilerin bağlamını daha uzun vadede tutabilir. Bu sayede:

  - Non-lineer ilişkileri öğrenebilir,

  - Atmosferik ve çevresel koşullardan doğan belirsizlikleri modelleyebilir,

  - Önceden tanımlanmamış anomalilere karşı esnek olabilir.

Projemde Kalman filtresi ile ön işlem yapılmış veriler, ardından LSTM'e beslenmiştir. Böylece:

  - Kalman filtresiyle gürültü azaltılmış ve veriler yumuşatılmış,

  - LSTM modeli de geçmiş örüntülerden öğrenerek daha güçlü konum tahminleri yapmıştır.

  - Bu hibrit yaklaşım, yalnızca fiziksel modellemeye ya da yalnızca veri öğrenimine dayalı yöntemlerden daha üstün performans sağlamıştır.

Kısaca: Kalman daha iyi filtreler, LSTM daha iyi tahmin eder. Bu projede ikisini birleştirmek, her iki yöntemin güçlü yanlarını bir araya getirmiştir.

`Mahalanobis mesafesi`, özellikle çok değişkenli verilerde aykırı değerleri tespit etmek ve ölçümler arası ilişkileri dikkate almak için kullanılan güçlü bir istatistiksel ölçüttür. Kalman filtresi gibi durum tahmini algoritmalarında, ölçümün modele ne kadar uyumlu olduğunu belirlemek için sıklıkla kullanılır.

Mahalanobis mesafesi aslında çok değişkenli bir "z-skoru" gibidir. Örneğin:

 - Mahalanobis mesafesi = 1 → Ortalama değerin bir standart sapma uzağında

 - Mahalanobis mesafesi = 3 → Ortalama değerin üç standart sapma uzağında

 - Mahalanobis mesafesi = 30 → Aykırı uçta, çok uzak bir ölçüm

Kalman filtresi bir tahmin yürütür ve dışarıdan gelen gözlemlerle bu tahmini düzeltir. Ancak gözlemler bazen hatalı olabilir. İşte Mahalanobis mesafesi, “Bu gözlem beklediğimden çok mu farklı?” diye soran bir dedektördür. Eğer cevap “Evet, çok farklı” ise gözlemi dikkate almaz.

Bu Projedeki Rolü (Kalman Filtresinde)
 - Kalman filtresi her ölçümde bir tahmin yapar ve bu tahmine gelen gerçek ölçümü karşılaştırarak güncelleme yapar.

 - Eğer gelen ölçüm, tahmin edilen değerden çok fazla sapmışsa (yani Mahalanobis mesafesi büyükse), bu ölçüm "aykırı" kabul edilir.

 - Belirlenen eşik (örneğin sigma_mahalanobis = 30.0) bir güven seviyesi belirler. Bu değerin üstünde kalan ölçümler filtreye dahil edilmez.

 - Mahalanobis mesafesi sayesinde Kalman filtresi bu anormal değerleri ayıklayabilir, bu da daha sağlam ve kararlı tahminler sağlar.

 - Bu sayede Kalman filtresi, yanlış/bozulmuş GNSS ölçümlerinden etkilenmeden çalışabilir.
"""

# Kalman filter
def Kalman_filter(zs, us, phone):
    """
    Args:
        zs : konum ölçümleri (her gözlem 3 bileşenden oluşacak -> x y z)
        us : hız bilgisi  (her gözlem 3 bileşenden oluşacak -> x y z)
        phone : kullanılan telefon modeli
    """
    # TANIMLAMALAR:

    # Hızın Standart Sapması
    sigma_v = 0.6 if phone == 'XiaomiMi8' else 0.1 # Xiomi marka telefonlarda daha yüksek bir hata payı bırakıyoruz. Çünkü bu telefonlarda dah büyük hata payı olduğu varsayılıyor
    # Konumun Standart Sapması
    sigma_x = 5.0  # (metre)
    # Mahalanobis mesafesini belirliyoruz. (aslında kalman filtremizdeki eşik değerimiz gibi düşünebiliriz)
      # Mahalanobis mesafesi, gözlemlenen değerlerin mevcut modelden ne kadar farklı olduğunu ölçen bir istatistiksel ölçüt.
      # Kalman fitrlemenin geri beslemesinde eğer gözlem bu değerden büyükse aykırı gözlem olarak kabul edilecek ve filtreleme sürecine dahil edilmeyecek
      # Yani amacı, aykırı gözlemleri filtrelemek bir nevi
    sigma_mahalanobis = 30.0

    n, dim_x = zs.shape # Konum ölçümlerinin sayısı = n , durum vektörünün boyutu = dim_x (her ölçüm için 3 boyutlu olacak dolayısıyla dim_x = 3 olacak)
    F = np.eye(3)  # Geçiş matrisimiz (sisteminin durum geçişlerini tanımlayacak). 3x3'lük birim matrisi oluşturduk.
    Q = sigma_v**2 * np.eye(3)  # İşlem gürültüsü matrisimiz. Hızın standart sapmasının karesini F (geçiş matirisi) ile çarpıyoruz.
                                # Sİstemde beklenen gürültü miktarını temsil edecek. Zamanla biriken hataları modellemede kullanacağız

    H = np.eye(3)  # Sistemin gerçek durumunu ölçümlerle ilişkilendirmek için 3x3'lük birim matrisimiz (Ölçüm Matrisi)
    R = sigma_x**2 * np.eye(3)  # Ölçüm gürültüsü matrisimiz. Konumun standart sapmasının karesini birim matris ile çarpıyoruz.
                                # Ölçümlerin ne kadar gürültülü olduğunu veya ne kadar güvenilir olduğunu belirtecek. R ne kadar küçükse o kadar güvenilirdir.

    # Başlangıç ​​durumu ve kovaryans
    x = zs[0, :3].T  # başlangıç durumumuz (konum vektörünün transpozu)
    P = sigma_x**2 * np.eye(3)  # başlangıçtaki kovaryans yani belirsizlik
    I = np.eye(dim_x) # birim matrisimiz

    x_kf = np.zeros([n, dim_x]) # her adımında elde edilen durum vektörlerinin sırasıyla saklayacağımız dizi (başlangıç değeri = 0)
    P_kf = np.zeros([n, dim_x, dim_x])  # her adımında elde edilen kovaryans matrislerini sırasıyla saklayacağımız dizi (başlangıç değeri = 0)




    # KALMAN FİLTRELEME yapabiliriz artık:
    for i, (u, z) in enumerate(zip(us, zs)):
        # İlk adımımız
        if i == 0:
            x_kf[i] = x.T # ilk durumu kaydettik
            P_kf[i] = P # ilk kovaryansı kaydettik -> sistemin güvenilirliğini ölçecek
            continue

        # Tahmin adımı
        x = F @ x + u.T # durumla ilgili yeni tahminimiz F@x + hızın transpozesi
        P = (F @ P) @ F.T + Q # bu tahminin kovaryansı. Ne kadar güvenilir olduğu

        # Mahalonabis mesafesini kullanarak aykırı değerleri kontrol ediyoruz. Eşik değerimizi yukarıda tanımlamıştık
        d = distance.mahalanobis(z, H @ x, np.linalg.pinv(P)) # z gözlemimiz , H@x tahminimiz , np.linalg.pinv(P) kovaryans matrisimiz(güvenilirliği ölçecek)

        # Güncelleme adımı.
        if d < sigma_mahalanobis: # Eğer mesafemiz en başta belirlediğimiz mahalonabis değerinden düşükse yani gözlem aykırı DEĞİLSE
            y = z.T - H @ x # gözlemimizle tahminimiz arasındaki fark
            S = (H @ P) @ H.T + R # sistemin güvenilirliğini ölçen kovaryans matrisi
            K = (P @ H.T) @ np.linalg.inv(S)  # kalman kazancımız. gözlemlerle tahminler arasındaki farkı ne kadar dikkate alacağımızı belirleyen bir katsayı
            x = x + K @ y # güncellenmiş durum vektörü
            P = (I - (K @ H)) @ P # başlangıçtaki kovaryans matrisini güncelliyoruz
        else: # aykırı ise
            P += 10**2*Q   # kovaryans matrisini artırarak gözlemlerle ilişkisini azaltacağız. Yani daha az güvenilirdir diyeceğiz

        x_kf[i] = x.T # durum vektörlerimiz
        P_kf[i] = P   # kovaryans vektörlerimiz

    return x_kf, P_kf





# KALMAN PÜRÜZSÜZLEŞTİRME
# İleri + geri Kalman filtresi ve pürüzsüzleştirme
  # Bu sayede verileri daha da hassaslaştırarak konum ve hız verilerinin doğruluğunu artıracağız

def Kalman_smoothing(x_wls, v_wls, phone):
    """
    Args:
        x_wls : Konum vektörü
        v_wls : Hız vektörü
        phone : Kullanılan telefon modeli
    """

    n, dim_x = x_wls.shape # aynı üstteki gibi konum vektörlerinin sayısı = n , durum vektörünün boyutu = dim_x (her ölçüm için 3 boyutlu olacak dolayısıyla dim_x = 3 olacak)

    # İleri filtreleme
    v = np.vstack([np.zeros([1, 3]), (v_wls[:-1, :] + v_wls[1:, :])/2]) # hız verilerinin ortak noktasını bulur (iki ardalık veri arasında)
    x_f, P_f = Kalman_filter(x_wls, v, phone) # Kalman filtresini çağırarak konum ve kovaryans tahmini yapıyoruz

    # Geri filtreleme
    v = -np.flipud(v_wls) # geri filtreleme yapacağımız için hız verilerinin sırasını tersine çevirdik
    v = np.vstack([np.zeros([1, 3]), (v[:-1, :] + v[1:, :])/2])
    x_b, P_b = Kalman_filter(np.flipud(x_wls), v, phone)  # Yine Kalman filtresini çağırdık. Bu sefer geçmişteki verileri göz önüne alarak daha doğru bir tahmin yapacağız

    # Pürüzsüzleştirme
    x_fb = np.zeros_like(x_f) # Hem ileri hem de geri yönde düzeltilmiş konum tahminleri
    P_fb = np.zeros_like(P_f) # Hem ileri hem de geri yönde düzeltilmiş hız tahminleri
    for (f, b) in zip(range(n), range(n-1, -1, -1)):
        P_fi = np.linalg.inv(P_f[f])  # ileri kovaryans matrisinin tersi
        P_bi = np.linalg.inv(P_b[b])  # Geri kovaryans matrisinin tersi

        P_fb[f] = np.linalg.inv(P_fi + P_bi)  # ileri ve geri kovaryan matrislerinin toplamının tersi. Bu sayede ileri ve geriyi birleştirdik
        x_fb[f] = P_fb[f] @ (P_fi @ x_f[f] + P_bi @ x_b[b]) # ileri ve geri tahminleri birleştirdik ve pürüzsüzleştirmeyi yapmış olduk

    return x_fb, x_f, np.flipud(x_b)  # ileri-geri konum tahmini , ileri konum tahmini ve geri konum tahminini geri döndürdük

"""# 9- Veri Setimize Bu Ön İşleme Tekniklerinin Uygulanması"""

import os

# Ana veri seti klasörüm
root_path = "/content/drive/MyDrive/BitirmeProjesi/smartphone-decimeter-2022/train"

# Sürüş oturumlarını listelelemek için bir for döngüsü yazacağız
drive_sessions = [d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))]
print("Tüm sürüş oturumları:", drive_sessions)

# Örnek olarak bir sürüş oturumunu seçelim
selected_drive = "2021-07-19-US-MTV-1"  # Bunu dinamik hale getirebiliriz

# Ve bu seçilen sürüş oturumundaki telefonları listeleyelim
drive_path = os.path.join(root_path, selected_drive)
phones = [p for p in os.listdir(drive_path) if os.path.isdir(os.path.join(drive_path, p))]

print(f"{selected_drive} oturumundaki telefonlar:", phones)

"""Oturum Açıklamaları:

- MTV (Mountain View, CA) → Google’ın ana merkezinin bulunduğu bölge, Silikon Vadisi’nde yer alır.
- SJC (San Jose, CA) → San Jose kodu, yine Silikon Vadisi bölgesinde yer alır.
- SFO (San Francisco, CA) → San Francisco.
- SVL (Silicon Valley, CA) → Silikon Vadisi genelini kapsayan bir kod.
- LAX (Los Angeles, CA) → Los Angeles kodu.
"""

# Üstteki 2 kodu birleştirerek daha açıklayıcı bir liste oluşturalım:

# Tüm sürüş oturumlarını alalım
drive_sessions = [d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))]

# Her oturum için telefonları getirelim
all_data = {}

for drive in drive_sessions:
    drive_path = os.path.join(root_path, drive)
    phones = [p for p in os.listdir(drive_path) if os.path.isdir(os.path.join(drive_path, p))]
    all_data[drive] = phones

# Sonuçları yazdıralım
for drive, phones in all_data.items():
    print(f"{drive} oturumundaki telefonlar: {phones}")

# Kullanıcının istediği veri setini seçmesini isteyelim
selected_drive = input("Hangi sürüş oturumunu kullanmak istiyorsunuz? ")
selected_phone = input("Hangi telefonu kullanmak istiyorsunuz? ")

# Veri Setimizin bulunduğu path'i oluşturuyoruz
path = os.path.join(root_path, selected_drive, selected_phone)

print(f"Seçilen veri yolu: {path}")

# Drive'ımızda depoladığımız GNSS ölçüm veri seti ile gerçek konum verilerini ieçeren dataframe'lerimiz
gnss_df = pd.read_csv(f'{path}/device_gnss.csv')  # GNSS verileri
gt_df = pd.read_csv(f'{path}/ground_truth.csv')  # Gerçek veriler



# GNSS verilerimize yazdığımız 'point_positioning' fonksiyonumuzu uygulayarak kullanıcının konum ve hız tespitini yapıyoruz
utc, x_wls, v_wls = point_positioning(gnss_df)

# Konum ve hız vektörlerindeki aykırı ya da NaN değerleri yazdığımız "exclude_interpolate_outlier" fonksiyonu ile interpolasyonla tespit ediyoruz ve dolduruyoruz
x_wls, v_wls = exclude_interpolate_outlier(x_wls, v_wls)

# Yazdığımız "Kalman_smoothing" fonksiyonu ile hız ve konum tahminimizin doğruluğunu artırıyoruz
x_kf, _, _ = Kalman_smoothing(x_wls, v_wls, selected_phone)

# Veri setini ECEF koorinat sisteminden Jeodezik Koor. Sis. çevirerek enlem, boylam ve yükseklik sistemine dönüştürüyoruz
# 10. bölümde bunları kullanacağız
llh_wls = np.array(pm.ecef2geodetic(x_wls[:, 0], x_wls[:, 1], x_wls[:, 2])).T # "point_positioning"den elde edilen hız ve konum koordinatları
llh_kf = np.array(pm.ecef2geodetic(x_kf[:, 0], x_kf[:, 1], x_kf[:, 2])).T # kalman filtresi sonucu iyileştirilen hız ve konum koordinatları

# Gerçek verilerle (gt_df) karşılaştırmak için fonksiyonlar sonucu ortaya çıkacak "gnss_df"imizi hazırlayarak referans noktası (base line) oluşturuyoruz
x_bl = gnss_df.groupby('TimeNanos')[['WlsPositionXEcefMeters', 'WlsPositionYEcefMeters', 'WlsPositionZEcefMeters']].mean().to_numpy() # GNSS verilerinde her zaman dilimi için pozisyon tahminlerinin ortalaması
llh_bl = np.array(pm.ecef2geodetic(x_bl[:, 0], x_bl[:, 1], x_bl[:, 2])).T # üstteki tahmini enlem, boylam ve yükseklik formatına dönüştürdük

"""# 10- Karşılaştırmak İçin Skor Hesaplanması ve Skorların Yorumlanması

Vincenty Formülü, bir kürenin yüzeyindeki iki nokta arasındaki mesafeyi hesaplamak için kullanılan yöntemdir.
<br></br>

<h3 style="color:blue;">Skorların Yorumlanması</h3>

1. **Referans Noktası Skoru (2.1929 metre):**

- Bu skor, temel modelin performansını gösterir.
- Düşük doğruluk beklediğimiz bir referans olarak düşünebiliriz.
<br></br>

2. **WLS Skoru (1.9769 metre):**

- Weighted Least Squares (WLS), GNSS verilerindeki hataları azaltmak için kullandığımız bir yöntemdi zaten.
- Baseline skorundan daha düşük bir değer, bu yöntemin ile birlikte pozisyon doğruluğunu artırdığımızı gösterir.
- Bu yöntemle elde ettiğimiz skorumuz Baseline'a göre yaklaşık %9.85 iyileşme gösteriyor.
<br></br>

3. **Kalman Filtresi Skoru (1.1443 metre):**

- Kalman filtresi, zaman serisi verilerindeki hataları daha ileri düzeyde filtrelemesi ve pozisyon tahminlerini daha da iyileştirmesi için kullanmıştık.
- Bu skor genellikle en düşük olmalıdır.
- Eğer gerçek dünya uygulamalarında yüksek hassasiyet elde etmemiz gerekiyorsa (örneğin, otonom araçlar, navigasyon sistemleri), Kalman filtresiyle elde ettiğimiz sonuçlar daha kritiktir.
- Kalman Filtresi ile elde ettiğimiz skor WLSE'e göre %42.12 , Baseline'a göre ise yaklaşık %47.82  daha iyi bir sonuç sağlıyor.
<br></br>

Genel olarak baktığımızda; sonuçlar, her adımda modelin doğruluğunun arttığını ve Kalman filtresi ile iyi bir doğruluk seviyesine ulaştığımızı gösteriyor.

"""

# Bu fonksiyon vincenty yöntemini kullanarak iki enlem ve boylam noktasını arasındaki mesafeyi hesaplayacak
def vincenty_distance(llh1, llh2):
    """
    Args:
        llh1 : İlk noktanın enlem ve boylam bilgisi (derece cinsinden)
        llh2 : İkinci noktanın enlem ve boylam bilgisi (derece cinsinden)
    Returns:
        d : İki nokta arasındaki mesafe (m)
    """
    d, az = np.array(pmv.vdist(llh1[:, 0], llh1[:, 1], llh2[:, 0], llh2[:, 1])) # PyMap kütüphanesindeki vincenty uzaklık formülünü kullandık mesaheyi hesaplamak için
                                                                                # buradaki az -> azimut açısı = bir noktanın diğer bir noktaya olan yönünü belirten bir açı ...
                                                                                # ... iki nokta arasındaki yönü bulmak için kullanacağız. Şu an burada kullanmadık

    return d





# Bu fonks ile GNSS ölçümleriyle gerçek ölçümleri (ground_truth) karışılştırarak skoru hesaplayacağız
def calc_score(llh, llh_gt):
    """
    Args:
        llh : Tahmin edilen enlem ve boylamlar (derece cinsinden)
        llh_gt : Gerçek enlem ve boylamlar ("ground truth")
    Returns:
        score : (m)
    """
    d = vincenty_distance(llh, llh_gt)  # tahmin ve gerçek konum arasındaki uzaklığı tuttuk
    score = np.mean([np.quantile(d, 0.50), np.quantile(d, 0.95)]) # 0.5'lik çeyreklik -> tipik hatayı gösterecek
                                                                  # 0.95'lik çeyreklik -> yüksek hataları (GPS sinyali kötü olan bölgelerde ise) gösterecek
                                                                  # Bu ikisinin ortalamasını alarak hem ortalama doğruluğu hem de kötü senaryoları aynı anda göz önünde bulunduracağız

    return score




# Gerçek konum verilerimizi Enlem ve Boylam olarak alarak karşılaştırmak için numpy'a dönüştürüyoruz
llh_gt = gt_df[['LatitudeDegrees', 'LongitudeDegrees']].to_numpy()

# GNSS verilerimizin skorlarını gerçek verilere olan uzaklığını bulacağız
vd_bl = vincenty_distance(llh_bl, llh_gt) # referans noktamızla (ön işlemeler sonucu) gerçek tahminler arası uzaklık
vd_wls = vincenty_distance(llh_wls, llh_gt) # Dayanıklı Ağırlıklı En Küçük Kareler Yönemi (point_positioning sonucu) ile gerçek tahminler arası uzaklık
vd_kf = vincenty_distance(llh_kf, llh_gt) # Kalman Filtresi sonucu ile gerçek tahminler arası uzaklık

# Bu 3 uzaklığın da ayrı ayrı skorunu hesaplayacağız
score_bl = calc_score(llh_bl, llh_gt) # Referans Noktası (Baseline) skoru
score_wls = calc_score(llh_wls, llh_gt) # WLS skoru
score_kf = calc_score(llh_kf[:-1, :], llh_gt[:-1, :]) # Kalman filtresi skoru

print(f'Referans Noktası (Baseline) Skoru: {score_bl:.4f} [metre]')
print(f'WLS Yöntemi Skoru: {score_wls:.4f} [metre]')
print(f'Kalman Filtresi skoru: {score_kf:.4f} [metre]')

"""# 11- Konum ve Hız Hatalarının Görselleştirilmesi

<h3>Grafiklerin Yorumlanması</h3>

- İlk Grafik

Bu grafikte üstte skorunu hesapladığımız üç farklı yöntemin mesafe hatalarını karşılaştırılıyoruz ve görselde her bir yöntemin performansını detaylı bir şekilde görebiliyoruz.

  - Referans Noktası (Baseline): Mavi çizgi ile gösteriliyor. Herhangi bir iyileştirme veya filtreleme uygulanmamış ham verilerin mesafe hatasını temsil ediyor.

  - WLS Yöntemi: Turuncu çizgi ile gösteriliyor. Ağırlıklı en küçük kareler (Weighted Least Squares) yöntemi ile iyileştirilmiş tahminler içeriyor.

  - Kalman Filtresi: Yeşil çizgi ile gösteriliyor. WLS tahminlerini Kalman filtresiyle daha da iyileştiriyor.

Baseline ve WLS yönteminde gözlenen ani sıçramalar, GNSS ölçümlerindeki gürültü ve aykırı değerlerden kaynaklanıyor şeklinde bir yorumda bulunabiliriz. Ancak Kalman filtresi ile bu sıçramaları etkin bir şekilde filtreleyerek en iyi skoru almaya daha da yaklaşıyoruz.

WLS yöntemi, ham veriye göre bir iyileşme sağlıyor ancak aykırı değerlere duyarlılığı nedeniyle bazı veri noktalarında yüksek hata gösteriyor. Kalman filtresinin eklenmesiyle hatalar daha düzenli ve tutarlı hale gelmiş.
<br></br>

- İkinci Grafik

Bu grafikte, tahmin ettiğimiz hızlar ile gerçek hızlar arasındaki farkın oldukça düşük ve tutarlı olduğu görünüyor. Bu, da sistemimizin genel olarak güvenilir bir şekilde çalıştığını destekliyor.

ELde ettiğimiz 0.1265'lik bir RMSE değeri de tahminlerin büyük oranda doğru olduğunu destekliyor.

Zaman zaman grafikte ani yükselişler olduğunu gözlemleyebiliriz. Bunun sebeplerini basitçe şu şekilde açıklayabiliriz:
  - GPS sinyallerindeki kayıplar (ağaçlık yerler, yüksek binalar)
  - Aracın hızındaki ani değişimler
  - Aracın ani manevraları

  Bu hataları azaltmak için zaten Kalman Filtresi , WLS gibi yöntemler ayrıca interpolasyon gibi eksik verileri doldurma gibi ön işleme uygulamaları yapmıştık. Ancak modelin doğruluğunu daha da artırmak için LSTM, ANN , RNN gibi makine ve derin öğrenme algoritmaları da kullanabiliriz.

"""

# Skorlarımızı görselleştirerek mesafe hatasını gösterelim
plt.figure(figsize=(15,6))
plt.title('Mesafe hatası')
plt.xlabel('Zaman [saniye]')
plt.ylabel('Mesafe hatası [metre]')
plt.plot(vd_bl, label=f'Referans Noktası (Baseline) Skoru: {score_bl:.4f} m')
plt.plot(vd_wls, label=f'WLS Yöntemi Skoru: {score_wls:.4f} m')
plt.plot(vd_kf, label=f'WLS Yöntemi Skoru + Kalman Filtresi skoru: {score_kf:.4f} m')
plt.legend()
plt.grid()
plt.ylim([0, 10])

# Hızın hatasını hesaplayalım
speed_wls = np.linalg.norm(v_wls[:, :3], axis=1)  # 4. kısımda oluşturduğumuz ve hız tahminlerini tuttuğumuz v_wls matrisinin ilk 3 sütununuu alarak hız bileşenlerini tutuyoruz (xyz)
                                                  # np.linalg.norm: her bir vektörün büyüklüğünü (normunu) hesaplayacak. Burada, üç boyutlu hız vektörünün büyüklüğünü hesaplanarak skaler bir hız değerine dönüştüreceğiz.
speed_gt = gt_df['SpeedMps'].to_numpy() # gerçek hız verilerimizi tutuyoruz
speed_rmse = np.sqrt(np.sum((speed_wls-speed_gt)**2)/len(speed_gt)) # ilk başta tuttuğumuz tahmini verilerle ikinci olarak tututğumuz gerçek değerler arasındaki farkımız

# Hesapladığımız hız hatasını görselleştirelim
plt.figure(figsize=(15,6))
plt.title('Hız hatası')
plt.xlabel('Zaman [saniye]')
plt.ylabel('Hız Hatası [m/s]')
plt.plot(speed_wls - speed_gt, label=f'Hız Hatasının En Küçük Kareler Hata Oranı (RMSE): {speed_rmse:.4f} m')
plt.legend()
plt.grid()

"""# 12- WGS Koordinat Sisteminden UTM Kooridinat Sistemine Dönüşüm

Bu fonksiyon ile birlikte daha önce kullandığımız WGS84 koordinat sistemi (dünya genelinde kullanılan enlem-boylam tabanlı sistem) ile ifade edilen coğrafi konumları, UTM (Universal Transverse Mercator) koordinat sistemine dönüştüreceğiz. Bunu yapammızdaki amacımızı şu şekilde açıklayabiliriz:

- WGS84, küresel bir alanı refarans alırken UTM, düzlemsel alanları referans alır. Yani dünya yüzeyini bölgelere ayırır ve her bökge için düzlem koordinatları sağlar. Bu da daha hassas mesafe ve alan hesaplaması yapmamızı sağlar.

UTM, dünyayı 60 adet 6 derecelik dilime (boylama) böler. Bu dilimler kuzey ve güney yarımküreye göre farklı EPSG kodları ile ifade edilecek:
- Kuzey yarımküre için kodlar 326XX
- Güney yarımküre için kodlar 327XX

  Örneğin:
  - (lon=30, lat=40) için 32636 dönecek. (kuzey yarım kürede - 326xx olduğunu anlayabiliriz)
  - (lon=-70, lat=-20) için 32719 dönecek. (güney yarım kürede - 327xx olduğunu anlayabiliriz)
"""

def convert_wgs_to_utm(lon, lat):
    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)
    if len(utm_band) == 1:
        utm_band = '0'+utm_band
    if lat >= 0:
        epsg_code = '326' + utm_band
    else:
        epsg_code = '327' + utm_band
    return epsg_code

"""# 13- Gerçek Veri Kümemizle Oluşturduğumuz Veri Kümelerininin  (llh_bl, llh_wls, llh_kf) Birleştirilmesi"""

# Hem baseline hem wls hem de kalman filtresi ile oluşturduğumuz df'lerimizden enlem , boylam ve hata(gürültü) sütunları içeren df'ler oluşturacağız
llh_bl_df = pd.DataFrame(llh_bl)
llh_bl_df.columns = ['lat', 'lon', 'Noise']

llh_wls_df = pd.DataFrame(llh_wls)
llh_wls_df.columns = ['lat', 'lon', 'Noise']

llh_kf_df = pd.DataFrame(llh_kf)
llh_kf_df.columns = ['lat', 'lon', 'Noise']

# Ground Truth yani gerçek verilerin olduğu veri setimizdeki EnlemDerecesi ve Boylam Derecesi sütunlarının ismini değiştrdik
gt_df.rename(columns = {'LatitudeDegrees':'lat', 'LongitudeDegrees':'lon'}, inplace = True)


all_tracks = pd.concat([gt_df, llh_bl_df, llh_wls_df, llh_kf_df]) # Daha sonra ilk olarak üstte düzenlediğimiz 4 df'i birleştirdik
all_tracks  =all_tracks[['lat', 'lon']]   # bu birleştirdiğimiz df'te sadece enlem ve boylam verilerini tuttuk

# Son oluşturduğumuz all_tracks df'imizde hangi verinin hangi yöntemden geldiğini anlamak için "Name" adında bir sütun oluşturacağız
all_tracks['Name'] = np.repeat(['gt','bl', 'wls','kf'],gt_df.shape[0])
all_tracks

# Yukarıda oluştruduğumuz total veri kümesini görselleştirmeye çalışalım

fig = px.line_mapbox(all_tracks,lat="lat",  lon="lon",  color="Name",  labels={"Name": "Veri Kaynağı"},zoom=12, center={"lat": 37.45 , "lon": -122.27},  height=600, width=1000)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show()

"""# 14 - Veri Setine Uzaklık , Hız , Açı, Eğiklik ve İvme Özelliklerinin Eklenmesi


Bu fonksiyon ile birlikte bir DataFrame'e (biz llh_kf_Df'i kullanacağız çünkü en iyi sonuç orda) uzaklık, hız, açı, eğrilik (curvature) gibi bazı  özellikleri ekleyecek.

Burada "eğrilik"ten kastımız yoldun düzlüğü ya da kıvrıklığıdır. Yol ne kadar düzse eğrilik sıfıra yakın olacaktır.

Bu fonksiyonnu rota optimizasyonu gibi işlemlerde kullanabileceğiz. Örneğin, araçların hareket verilerini analiz etmek, rotadaki keskin dönüşleri veya hız değişimlerini belirlemek için kullanacağız.

(X ->türevi: v ->türevi: a)

Eğrilik Formülü: ![image.png](data:image/png;base64,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)
"""

def add_distance_speed_angle_curv_diff(df):
    # İlk Başta Yapılması Gerekli Dönüşüm
    # Normalde GPS koordinatları (en-boy)  kürsel koor. sis olan WGS84'te ifade edilir ancak eğrilik , hız gibi hesaplamak istediğimiz metrikler düzlemsel koor. sis.de daha kolay hesaplanır...
    # ... Bundan dolayı daha önce de yaptığımız gibi WGS84(epsg:4326) sisteminden UTM'ye geçiş yapmamız gerekiyor
    utm_code = convert_wgs_to_utm(df['lon'].mean(), df['lat'].mean())   # enlem ve boylamın ortalamasına göre bir utm kodu oluşturduk (wgs koor. sisteminden utm'ye geçiş yaptık)
    crs_wgs = proj.Proj(init='epsg:4326') # WGS84 koor sis. tanımladık
    crs_utm = proj.Proj(init='epsg:{0}'.format(utm_code)) # dönüşümü sağladık
    x, y = np.multiply(proj.transform(crs_wgs, crs_utm, df['lon'], df['lat']),0.001)  # metre yerine kilometre cinsine çevireceğimiz için 0.001 ile çarptık



    # Hız(vektörel) ve Sürat(skaler) Hesaplaması
    # np.gradient ile birlikte x ve y koordinatlarının zamana göre türevleri (değişim oranları , eğimlerini) alacağız.
    x_t = np.gradient(x)  # x eksenindeki değişim oranı.
    y_t = np.gradient(y)  # y eksenindeki değişim oranı.

    vel = np.array([ [x_t[i], y_t[i]] for i in range(x_t.size)])  # hız vektörel olduğu için hız vektörünü oluşturduk

    speed = np.sqrt(x_t * x_t + y_t * y_t)  # x ile y eksenindeki türevlerin karesinin toplamının karekökü alınarak toplam hızı hesapladık (pisagor).

    tangent = np.array([1/speed] * 2).transpose() * vel   # hızın yönünü hesaplamak için de teğet vektörü oluşturduk



    # Eğrilik Hesaplanması (Yolun düzlüğü ya da virajlılığı)
    ss_t = np.gradient(speed) # hızın zamana göre türevi yani ivme
    xx_t = np.gradient(x_t)   # xx_t aslında x'in 2. derece türevi ya da x_t'nin türevi YANİ # x ekseninin ikinci türevi
    yy_t = np.gradient(y_t)   # y ekseninin ikinci türevi

    curvature = (xx_t * y_t - x_t * yy_t) / (x_t * x_t + y_t * y_t)**1.5  # eğrilik formğlğ kullanarak eğriliği hesapladık
    xxyy = np.sqrt(xx_t * xx_t + yy_t * yy_t) # bu eğrilik değişiminin büyüklüğünü hesaplıyoruz



    # İvme Hesaplanması
    tangent_x = tangent[:, 0] # x ekseninin eğimi (tanjantı)
    tangent_y = tangent[:, 1] # y ekseninin eğimi

    # tangent_x = yolun türevi = hız. Burda da tangent_x'in türevinin türevini alarak ivmeyi bulacağız yani ivmeyi
    deriv_tangent_x = np.gradient(tangent_x)  # x eksenindeki ivmesi
    deriv_tangent_y = np.gradient(tangent_y)  # y eksenindeki ivmesi

    # x eksenindeki ivme ile y eksenindeki ivmeyi birleştireceğiz
    # Yani, her bir zaman adımındaki değişim şu şekilde bir vektör olarak ifade edeceğiz: dT_dt=[deriv_tangent_x,deriv_tangent_y]
    dT_dt = np.array([ [deriv_tangent_x[i], deriv_tangent_y[i]] for i in range(deriv_tangent_x.size)])

    length_dT_dt = np.sqrt(deriv_tangent_x * deriv_tangent_x + deriv_tangent_y * deriv_tangent_y) # üstteki dT_dt vektörünün büyüklüğünü hesapladık. Yani ivmenin büyüklüğünü hesapladık

    normal = np.array([1/length_dT_dt] * 2).transpose() * dT_dt  # Normal vektörümüz =  dT_dt/|dT_dt|

    t_component = np.array([ss_t] * 2).transpose()  # Tanjant bileşenizimiz = ss_t * tanjant yön vektörü. Hareket yönündeki hız değişimini temsil edecek
                                                    # ss_t, hızın zamana göre türeviydi (ivme)

    n_component = np.array([curvature*ss_t * ss_t] * 2).transpose()   # Normal bileşenimiz(ivmenin normal yönündeki bileşeni) =  curvate * ss_t * ss_t * normal vektör.
                                                                      # Hareketin eğrilik veya dönme kaynaklı bileşeni

    # TOPLAM İvme
    accel = t_component * tangent + n_component * normal


    # Oluşturduğumuz bileşenleri kullanmak istediğimiz df'e sütun olarak ekleyelim
    df['curve']= curvature
    df['speed']= speed
    df['acc'] = np.linalg.norm(accel, axis=1)
    df['gas']= ss_t
    df['dir_x'] = x_t / speed
    df['dir_y'] = y_t / speed

    return df

"""#15- Hız , Açı , Eğiklik ve İvme Özelliklerinin Veri Setine Entegresi Ve Görselleştirmeler"""

# Yukarıda oluşturduğumuz add_distance_speed_angle_curv_diff fonksiyonumuzu llh_kf veri setimize entegre edelim.
# Bu sayede Hız, İvme, Hız Değişim Oranı , Eğrilik ve Yön gibi bileşenlerle veri setimizi zenginleştirelim

llh_kf_df = pd.DataFrame(llh_kf)
llh_kf_df.columns = ['lat', 'lon', 'Noise'] # sütnlarımız aynı şekilde eylem, boylam ve gürültü olacak
llh_kf_df = add_distance_speed_angle_curv_diff(llh_kf_df) # add_distance_speed_angle_curv_diff fonksiyonumuzu çalıştırarak yeni  llh_kf_df veri setimizi oluşturduk

# Hız verimize bir medyan filtresi uygulayacağız signal.medfilt ile birlikte. medlift'in değerini 3 verdik. Bu da her 3 noktanın ortanca değerini alarak veriyi düzeltmesi anlamına gelecek
# Bu sayede gürültüyü azaltacak, hızdaki ani artış ve düşüşleri filtrelemeiş olacağız. Bu sayede grafikteki dalgalanmalar daha düzenli ve tutarlı hale gelerek analizimiz daha anlamlı olacak
speed = signal.medfilt(llh_kf_df['speed'],3)

llh_kf_df['speed_filt'] = speed   # Filtreden geçirdiğimiz hız verisini "speed;_filt" adında yeni bir sütuna atadık


# Orijinal Hızın grafiği
# plt.figure(5, figsize=(15, 3))
# plt.title("Orijinal Hız")
# plt.xlabel("Zaman [sn]")
# plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['speed'], label="Orijinal Hız")
# plt.legend()

# Yukarıda yaptığımız filtreleme sonucu Hızın grafiği
plt.figure(6, figsize=(15, 3))
plt.title("(Filtrelenmiş) Hız")
plt.xlabel("Zaman [sn]")
plt.ylabel("Hız Değerleri [m/sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['speed_filt'], label="Filtrelenmiş Hız")
plt.legend()



# YORUMLAMA
  # Arabanın hareketinin başladığı ve bittiği zaman aralıklarını net bir şekilde görebiliyoruz (örneğin hızın 0 olduğu noktalar)
  # Hızdaki değişimlere bakarak hızlanma ve yavaşlama bölgelerini belğirleyebiliriz.

# Hız'da yaptığımız işlemin aynısını yapıyoruz. İvmede de filtreleme işlemi yaparak gürültüden arındırmayı amaçlıyoruz
acc = signal.medfilt(llh_kf_df['acc'],3)

llh_kf_df['acc_filt'] = acc   # filtrelenmiş veriyi "acc_filt" adında bir sütuna atıyoruz

# Orijinal İvmenin grafiği
plt.figure(1, figsize=(15,3))
plt.title("Orijinal İvme")
plt.xlabel("Zaman [sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['acc'])
plt.legend()

# Filtelenmiş İvmenin grafiği
plt.figure(2, figsize=(15,3))
plt.title("(Filtrelenmiş)İvme")
plt.xlabel("Zaman [sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['acc_filt'])



# YORUMLAMA:
  # Yukarıdaki grafikte gördüğümüz gibi orijinal ivme grafiğinde ani zirveler ve düzensiz değişimler var. Biz bunu gidermek için medyan filter kullandık
  # İkinci grafikte gördüğümüz gibi filtrelememiz işe yaramış ve gürültü oranımız azalmış. Daha düzenli ve anlamlı bir grafik elde ettik. Dolayısıyla daha analiz edilebilir.

# Hız'da yaptığımız işlemin aynısını yapıyoruz. Hız değişiminde de filtreleme işlemi yaparak gürültüden arındırmayı amaçlıyoruz
gas = signal.medfilt(llh_kf_df['gas'],3)

llh_kf_df['gas_filt'] = gas  # filtrelenmiş veriyi "gas_filt" adında bir sütuna atıyoruz

# Orijinal Hız Değişim Oranı grafiği
# plt.figure(7, figsize=(15,3))
# plt.title("Orijinal Hız Değişim Oranı")
# plt.xlabel("Zaman [sn]")
# plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['gas'])

# Filtrelenmiş Hız Değişim Oranı grafiği
plt.figure(8, figsize=(15,3))
plt.title("(Filtrelenmiş) Hız Değişim Oranı")
plt.xlabel("Zaman [sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['gas_filt'])

crv = llh_kf_df['curve']

# Eğrilik değerleri aşırı yüksek ya da düşük olanları sınırlandıracağız. Burada 5 ve -5 değerlerini standart sapmaya göre verdik. Bu sayede aşırı değerlerin analizini, filtrelemesini gerçekleştirdik
crv[crv>10]=5
crv[crv<-10]=-5

# Hız'da yaptığımız işlemin aynısını yapıyoruz. Eğrilikde de filtreleme işlemi yaparak gürültüden arındırmayı amaçlıyoruz. Her 3 değerin ortanca değerini alıyoruz
crv = signal.medfilt(crv,3)

# Sigmoid fonksiyonu uyguladık. Buradaki amacamız: eğrilik verisini normalize ederek 0-1 arasına sıkıştırmak. Bu sayede daha tutarlı sonuçlar elde etmek
crv = 1/(1+np.exp(-crv))

# İkinci bir filtreleme yapıyoruz.
  # İlk filtreleme genellikle küçük ve hızlı değişimlerin etkisini azaltacak
  # İkinci filtrelemede Her 9 değerin ortanca değerini alarak kısa vadeli dalgalanmaları iyice düzeltecek. Sigmoid'ten kaynaklanan olası dalgalanmaları giderecek
crv = signal.medfilt(crv, 9)

llh_kf_df['crv_filt'] = crv   #  filtrelenmiş veriyi "crv_filt" adında bir sütuna atıyoruz

# Orijinal Eğrilik grafiği
plt.figure(3, figsize=(15,3))
plt.title("Orijinal Eğrilik")
plt.xlabel("Zaman [sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['curve'])

# Filtrelenmiş Eğrilik grafiği
plt.figure(4, figsize=(15,3))
plt.title("(Filtrelenmiş) Eğrilik")
plt.xlabel("Zaman [sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['crv_filt'])



# YORUMLAMA
  # Orijinal grafikte verinin gürültüden etkilendiği çok bariz görünüyor.
  # Gürültüleri büyük ölçüde azalttığımızı görüyoruz Daha düzenli ve analiz edilbilir bir veri seti elde ettik

# Yön

plt.figure(10, figsize=(15,5))
plt.title("Yön - X ve Y Yönündeki")
plt.xlabel("Zaman [sn]")
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['dir_x'] , label = "X yönündeki hız") # 14. bölümde oluştruduğumuz x yönündeki hareket
plt.plot(range(llh_kf_df.shape[0]), llh_kf_df['dir_y'] , label = "Y yönündeki hız") # 14. bölümde oluştruduğumuz y yönündeki hareket
plt.legend()

# Yorumlama:
  # Eğer çizgi pozitifse, o eksende ileri yönde hareket olduğu anlamına gelir.
  # Eğer çizgi negatifse, ters yönde hareket olduğu anlamına gelir.
  # Çizginin sıfıra yakın olduğu yerlerde, o eksendeki hareketin minimal olduğu çıkarılabilir.

"""* Görselleştirmeler"""

# Eğrilik değerlerimizi görselleştirelim
fig = px.scatter_mapbox(llh_kf_df,
                        lat="lat",
                        lon="lon",
                        color="crv_filt",
                        zoom=12,
                        center={"lat":37.45, "lon":-122.27},
                        height=600,
                        width=800)
fig.update_layout(mapbox_style='open-street-map')
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show()

# Yorumlama:
  # Her bir nokta, hareketin bir enlem ve boylam koordinatına karşılık gelir.
  # Noktaların renkleri, ilgili noktadaki eğrilik değerine göre değişecek.
    # sarı-kırmızı renkler: Daha yüksek eğrilik (keskin dönüş - viraj).
    # mavi-mor renkler: Daha düşük eğrilik (düz yol).
  # Bu çıktıyı rota analizinde kullanabiliriz

# Eğrilik ve İvme Değerlerini görselleştirelim katmanlı bir şekilde. Bu sayade bunlar arasındaki ilişkiyi daha iyi anlayabiliriz

# İlk katmanımız olan İvme katmanı
fig = go.Figure(go.Scattermapbox(lon=llh_kf_df['lon'],\
                                        lat=llh_kf_df['lat'],\
                                        marker= go.scattermapbox.Marker(size=6, showscale=True), opacity=0.6,
                                        marker_color=llh_kf_df['acc_filt'], name = 'acceleration'))


# İkinci katmanımız Eğrilik katmanı
fig.add_trace(go.Scattermapbox(lon=llh_kf_df['lon'],\
                                        lat=llh_kf_df['lat'],\
                                        marker= go.scattermapbox.Marker(size=6, showscale=True), opacity=0.6,
                                        marker_color=llh_kf_df['crv_filt'], name = 'curvature'))


# Noktaların üzerine geldiğimizdeki hover bilgilerini ayarlayalım
fig.update_traces(
    showlegend=True,
    hoverinfo='text',
    text=[
        f"Lat: {lat}, Lon: {lon}, Acc: {acc:.4f}, Crv: {crv:.2f}"
        for lat, lon, acc, crv in zip(llh_kf_df['lat'], llh_kf_df['lon'], llh_kf_df['acc_filt'], llh_kf_df['crv_filt'])
    ]
    )


# Sol tarafa katmanlar arasında geçişi kolaylaştırmak adına bir buton ekleyelim
fig.update_layout(
    updatemenus=[
        dict(
            buttons=[
                dict(
                    label="İvme",
                    method="update",
                    args=[{"visible": [True, False]}, {"title": "İvme"}]
                ),
                dict(
                    label="Eğrilik",
                    method="update",
                    args=[{"visible": [False, True]}, {"title": "Eğrilik"}]
                )
            ],
            direction="down",
            showactive=True
        )
    ]
)



# Estetik görünüş adına işaret tablosunda güzelleştirmeler yapalım
fig.update_layout(
    legend=dict(
        orientation="h",       # Yatay düzen için
        yanchor="bottom",
        xanchor="center",
        x=0.5,
        title_text="Renk Skalası"
    ),
    margin=dict(r=0, t=30, l=0, b=0),
    title=dict(
        text="Eğrilik ve İvme Değerlerinin Görselleştirilmesi",
        x=0.5,
        font=dict(size=18)
    )
)



# Haritamızın ayarlamalarını yapalım
fig.update_layout(
    mapbox_style="carto-positron",  # sade ve okunaklı bir görünüm için bu stili kullanıdk
    autosize=True,
    hovermode='closest',
    mapbox=dict(
        accesstoken="pk.eyJ1IjoibGF5YWxoYW1tYWQiLCJhIjoiY2wzbWd5ZWxjMDFqNDNmcWt5MzRzNHdlaCJ9.B04CAbFi5Llmk2B78EP6JQ",
        bearing=0,
        center=dict(lat=37.45,lon= -122.27),
        pitch=0,  # yatay bakış açısı
        zoom=12)
)

fig.show()


# YORUMLAMA
  # İvme Değerleri:
    # İvme değerleri, genellikle trafik ışıkları veya keskin dönüşlerin yani virajların olduğu yerlerde daha yüksek olabilir.
    # Düşük ivme genellikle düz yollarda veya sabit hızla gidilen bölgelerde görülmesi beklenecek.

  # Eğrilik Değerleri:
    # Yüksek eğrilik (virajlar) genelde yolların kıvrımlı olduğu bölgelerde gözlemlenecek.
    # Düşük eğrilik (düz yollar), otoyollar veya uzun düz yollar boyunca gözlemlenecek.
    # Eğrilik değerleri için yaptığımız filtreleme işlemleri, haritadaki gürültüyü azaltmamızı sağlayacak.

  # Bu grafik sayesinde:
    # Yüksek eğrilik ve yüksek ivme bir aradaysa buralar kazaların sıkça yaşanabileceği bölgeler olarak iaşaretlenebilir
    # Araçların hızlanma ve eğrilik değerlerine bakarak sürüş dinamikleri anlaşılabilr (örneğin ivmelenme verileri trafik sıkışıklığını ya da ani hız değişimlerini gösterebilir)
    # Yolu tasarlayan ve trafikle ilgilenen mühendisleri, bu verileri kullanarak yol tasarımlarını iyileştirebilir veya sorunlu bölgeleri belirleyebilir

# KULLANDIĞIMIZ VERİ SETİNDE BANDIRMA YA DA TÜRKİYE İLE İLGİLİ VERİ BULUNMADIĞINDAN BANDIRMA İLE İLGİLİ BİR TAHMİNE BULUNAMIYORUZ

fig = px.line_mapbox(all_tracks,lat="lat",  lon="lon",  color="Name",  labels={"Name": "Veri Kaynağı"},zoom=12, center={"lat": 40.3506 , "lon": 27.9767},  height=600, width=1000)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show()

"""#16- LSTM Uygulamak İçin Veri Setinin Hazırlanması

Neden LSTM?

- GNSS verileri genellikle zaman serisi formatında olduğu için, LSTM/GRU gibi tekrarlayan sinir ağları (RNN-based models), geçmiş bilgileri hatırlayarak daha isabetli tahminler yapabilir.
- GPS sinyallerinin kaybolduğu veya gürültülü olduğu durumlarda, önceki verilere dayanarak tahmin yapabilir.
"""

# DERİN ÖĞRENMEYE SOKABİLMEK İÇİN BAĞIMLI VE BAĞIMSIZ DEĞİŞKENLERİMİZİ (x - y)  BELİRLEMEMİZ GEREKİYOR İLK BAŞTA


# llh_kf_df'den 'speed', 'curve', 'acc' ve 'gas' yani hız değişim oranı sütunlarını çıkarıyoruz ve bunu X_raw adında yeni bir veri setinde tutuyoruz.
# Çünkü bu sütunları tahmin için girdi olarak kullanmayacağız. Bunun yerine bir filtrelediğimiz hallerini kullanacağız
X_raw = llh_kf_df.drop(['speed','curve','acc','gas'], axis=1)



# Modelin geçmiş adımları dikkate alarak daha doğru tahminler yapmasını sağlamak için bir işlemde bulunacağız:
# tracelength ile geçmişte kaç zaman adımını dikkate alacağımızı belirliyoruz.
# Burada 4 adım seçtik, yani geçmiş 4 zaman adımına bakacağız.
# Bu değer bizim doğruluk değerimizi etkileyebilir. Burada ortalama bir değer olarak 4 kullanıldığı için 4 değerini seçtik
tracelength = 4

# X_raw'u başlangıç olarak X_trace'e atıyoruz.
# Daha sonra yukarıda belirttiğimiz miktarda geçmiş zaman adımlarıyla genişleteceğiz.
X_trace = X_raw

# Geçmiş zaman adımlarını X_trace'e eklemek için bir döngü başlatıyoruz.
for i in range(tracelength):
    # X_raw'u i kadar kaydırarak, geçmiş zaman adımlarını oluşturuyoruz.
    # Daha sonra bu kaydırılmış sütunları X_trace'e ekliyoruz.
    X_trace = pd.concat([X_trace, X_raw.shift(i)], axis=1)  # buradaki X_raw.shift(i); örneğin, i=1 için, veriyi 1 adım geri kaydırır ve böylece geçmiş verileri elde etmemizi sağlar.

# Yeni sütun isimlerini düzenliyoruz. Her sütun "Sütün0", "Sütün1", ..., "Sütün(N)" şeklinde isimlendirilir.
X_trace.columns = ["Sütün" + str(i) for i in range(0, X_trace.shape[1])]

# Kaydırma işlemi sonucu oluşlan boş hücreleri (NaN) dolduruyoruz. (örneğin, başlangıçtaki ilk 4 adımda geçmiş veriler eksik olur)
# Backfill yöntemiyle, bir sonraki geçerli değerle boşlukları dolduruyoruz. Bu sayede eksik veri problemini çözmüş oluyoruz.
X_trace.fillna(method='backfill', inplace=True)



# Tahmin etmek istediğimiz bağımlı değişkenleri (enlem ve boylam) seçiyoruz.
y = gt_df[['lat', 'lon']]

X_raw.iloc[:,:]

X_raw.iloc[:,3:]

# NORMALİZASYON işlemimizi uygulayalım

# Verileri -1 ile 1 arasına ölçeklendirebilmek için MinMaxScaler'ı kullanıyoruz.
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range=(-1, 1))

# X_raw ve gt_df[['lat', 'lon']] verilerini numpy array formatına çeviriyoruz.
x_data = np.array(X_raw)
y_data = np.array(gt_df[['lat', 'lon']])

# X verilerini ölçeklendiriyoruz (yani -1 ile 1 arasına çekiyoruz).
# Bu adım, modelimizin daha verimli öğrenmesine yardımcı olacak.
new_x_data = sc.fit_transform(x_data)

# X verisinin boyutlarını yeniden şekillendiriyoruz çünkü Derin öğrenme modelleri (özellikle LSTM gibi modeller) genellikle 3D bir yapı bekliyor.
# Bu yüzden veriyi (örnek sayısı, özellik sayısı, 1) şeklinde yeniden düzenliyoruz.
new_x_data = new_x_data.reshape(x_data.shape[0], x_data.shape[1], 1)

# Bağımlı değişken verilerini de aynı şekilde ölçeklendiriyoruz.
# Bu, hedef değişkenlerimizin (enlem ve boylam) aynı aralıkta olmasını sağlayacak.
new_y_data = sc.fit_transform(y_data)

# Y verisini daha tutarlı hale getirmek için, X verisinin ilk iki sütununu çıkarıyoruz.
# Bu sayede x ve y arasında anlamlı bir ilişki kurmaya çalışıyoruz (hizalama işlemi). Bunu yapmasaydık modeli eğitmemiz daha zor olacaktı çünkü x ve y arasında bir ilişki kurulması zorlaşırdı
# Derin öğrenme modellerinde x ve y arasındaki bu uyumu sağlamak önemli
new_y_data = new_y_data - new_x_data[:, 0:2, 0]



print("X_data'nın boyutu :", x_data.shape)
print("new_x_data'nın boyutu :", new_x_data.shape)

print("y_data'nın boyutu :", y_data.shape)
print("new_y_data'nın boyutu :", new_y_data.shape)

"""# 17- LSTM Modelinin Uygulanması"""

import tensorflow as tf  # TensorFlow kütüphanesini içe aktarıyoruz.

# Keras'ın model ve katmanlarını içe aktarıyoruz.
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers.recurrent import LSTM
from tensorflow.python.keras.layers.core import Dense, Activation, Dropout

# Sklearn kütüphanesinden veri ölçekleme, hata hesaplama ve veri bölme fonksiyonlarını içe aktarıyoruz.
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# Modeli oluşturuyoruz.
model = Sequential()

# İlk LSTM katmanını ekliyoruz.
# units=20 -> 20 LSTM hücresi kullanıyoruz.
# batch_input_shape=(None, new_x_data.shape[1], 1) -> Zaman adımı sayısını ve özellik boyutunu belirtiyoruz.
# return_sequences=False -> Bu katmandan yalnızca son çıktıyı alıyoruz.
model.add(LSTM(units=20, batch_input_shape=(None, new_x_data.shape[1], 1), return_sequences=False))


# İkinci katmanları (hidden layer) ekliyoruz. Ara katmanda relu kullanıyoruz ki kaybolan gradyan problemimiz olmasın. Relu, önemsiz nöronların kat sayısını 0 yapacak.
model.add(Dense(128, activation='relu'))  # 128 nöronlu ve ReLU aktivasyon fonksiyonlu bir katman ekliyoruz.
model.add(Dense(64, activation='relu'))   # 64 nöronlu bir katman ekliyoruz.
model.add(Dense(16, activation='relu'))   # 16 nöronlu bir katman ekliyoruz.
model.add(Dense(4, activation='relu'))    # 4 nöronlu katman ekliyoruz.

# Dropout katmanı ekleyerek overfitting'i önlüyoruz.
# model.add(Dropout(0.2))  # %20 oranında rastgele bağlantıları devre dışı bırakıyoruz ki aşırı öğrenme olmasın

# Çıkış katmanını ekliyoruz.
model.add(Dense(units=2))  # 2 nöronlu bir çıkış katmanı ekliyoruz (latitude, longitude).

# Modeli derliyoruz.
# loss='mean_squared_error' -> Ortalama kare hata (MSE) kullanarak kaybı hesaplıyoruz.
# optimizer='adam' -> Adam optimizasyon algoritmasını kullanıyoruz.
# metrics=['mae', 'mse' , 'accuracy'] -> Ortalama mutlak hata (MAE) ve MSE'yi ve accuracy'i performans ölçütü olarak kullanıyoruz.
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse' , 'accuracy'])


# Modelin özetini ekrana yazdırıyoruz.
model.summary()

# Tenserflow'da DistributedDatasetInterface özelliğinin bulunmadığı hatasını alıyordum. Onu çözmek için böyle bir yöntem buldum.

from tensorflow.python.keras.engine import data_adapter

def _is_distributed_dataset(ds):
    return isinstance(ds, data_adapter.input_lib.DistributedDatasetSpec)

data_adapter._is_distributed_dataset = _is_distributed_dataset

# Kuruduğumuz ağı normalize ettiğimiz veri setimize fit edelim
history = model.fit(new_x_data,new_y_data,epochs=5)


# history = model.fit(new_x_data,new_y_data,epochs=10) -> loss: 8.2575e-08 - mae: 2.1054e-04 - mse: 8.2575e-08 - accuracy: 0.8233
# history = model.fit(new_x_data,new_y_data,epochs=3) -> loss: 7.5112e-08 - mae: 1.9982e-04 - mse: 7.5112e-08 - accuracy: 0.8444
# history = model.fit(new_x_data,new_y_data,epochs=5 , batch_size=32) -> loss: 7.7622e-08 - mae: 2.0202e-04 - mse: 7.7622e-08 - accuracy: 0.8444
# history = model.fit(new_x_data,new_y_data,epochs=5 , batch_size=128) -> loss: 7.0668e-08 - mae: 1.9060e-04 - mse: 7.0668e-08 - accuracy: 0.8444

# "cannot import name '__version__' from 'tensorflow.python.keras' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/__init__.py)" hatası alıyordum. Bunu düzeltmek için böyle bir işlem yaptım

import tensorflow.python.keras as tf_keras
from keras import __version__
tf_keras.__version__ = __version__

model.save('/content/drive/MyDrive/BitirmeProjesi/sonuclar/model.h5')

from tensorflow.python.keras.models import load_model
model1_epochs5 = load_model('/content/drive/MyDrive/BitirmeProjesi/sonuclar/model.h5')

plt.plot(history.history['accuracy'])
plt.xlabel("İterasyon Sayısı")
plt.ylabel("Doğruluk Oranı")
plt.title("Eğitim Accuracy (Doğruluk) Oranı")
plt.show()

plt.plot(history.history['mse'])
plt.xlabel("İterasyon Sayısı")
plt.ylabel("Ortalama Kare Hata (MSE)")
plt.title("Eğitim Ortalama Kare Hatası (MSE) - Hata Fonksiyonu")
plt.show()

# Modelden tahmin edilen sonuçları alıyoruz.
# 'new_x_data' verisi üzerinde modelin tahminlerini yapıp, bir düzeltme uyguluyoruz (ilk iki kolon için).
result = model.predict(new_x_data) + new_x_data[:,0:2,0]

# Sonuçları ters dönüşüm yaparak orijinal ölçek değerlerine çeviriyoruz. (MinMax ile normalleştirmiştik tersini aldık. -1,1 arasına sıkıştırmıştık)...
# ... bunu yapmamızdaki sebep gerçek dünyada anlamlı veriler elde etmek...
# ... Örneğin, modelin tahminleri -1 ile 1 arasında olabilir ama bunları gerçek dünyadaki verilerle karşılaştırabilmek için veriyi ilk başta kullandığımız orijinal ölçeklerine geri döndürmeliyiz.
# 'sc' -> sc = MinMaxScaler(feature_range=(-1, 1))
res_df = sc.inverse_transform(result)

# Predicted (tahmin edilen) ve actual (gerçek) GPS koordinatlarını df'e çeviriyoruz.
# 'res_df' ve 'llh_gt' her bir satırda bir GPS koordinatı içeriyor.
# 'f0' ve 'f1' burada, veri çerçevesindeki ilk ve ikinci özellikler (enlem ve boylam) için etiketler.
predicted_df = pd.DataFrame(data=res_df[0:,0:], index=[i for i in range(res_df.shape[0])], columns=['f'+str(i) for i in range(res_df.shape[1])])
actual_df = pd.DataFrame(data=llh_gt[0:,0:], index=[i for i in range(llh_gt.shape[0])], columns=['f'+str(i) for i in range(llh_gt.shape[1])])




# Grafiği Görselleştirelim
# 'predicted_df' tahmin edilen GPS noktalarını temsil ediyor.
# 'actual_df' ise gerçek GPS noktalarını temsil ediyor.
plt.scatter(x=predicted_df['f0'], y=predicted_df['f1'], c='red', s=0.5, alpha=0.5, label="Tahmin Edilen")
plt.scatter(x=actual_df['f0'], y=actual_df['f1'], c='green', s=0.5, alpha=0.5, label="Gerçek Veri")

# Grafik başlığını ve eksen etiketlerini belirliyoruz.
plt.xlabel("Enlem (Latitude)")
plt.ylabel("Boylam (Longitude)")
plt.title("Tahmin Edilen vs Gerçek Değerler")

# Grafik üzerinde gösterilen veriler için etiketler ekleyip çizdiritoruz
plt.legend()





# YORUMLAMA:
  # Gerçek GPS Verileri (yeşil) , Modelin Tahminleri (kırmızı)

plt.plot(actual_df['f0'], actual_df['f1'], 'g-', label="Gerçek Yol")
plt.plot(predicted_df['f0'], predicted_df['f1'], 'r--', label="Tahmin Edilen Yol")
plt.xlabel("Enlem (Latitude)")
plt.ylabel("Boylam (Longitude)")
plt.title("Gerçek vs Tahmin Edilen Yol")
plt.legend()

import plotly.express as px

# Hata mesafelerini hesapla
error_distances = vincenty_distance(res_df, llh_gt)

# Hata mesafelerini DataFrame'e ekle
error_df = pd.DataFrame({'lat': llh_gt[:, 0], 'lon': llh_gt[:, 1], 'error': error_distances})

# Harita üzerinde hata dağılımını göster
fig = px.scatter_mapbox(error_df, lat="lat", lon="lon", color="error",
                        zoom=12, center={"lat": 37.45, "lon": -122.27},
                        height=600, width=1000,
                        color_continuous_scale=px.colors.sequential.Reds,
                        title="Hata Dağılımı")
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show()

# Gerçek GPS verisi (llh_gt) ile model tarafından tahmin edilen konum (res_df) arasındaki farkı hesaplayalım
vd_nn = vincenty_distance(res_df, llh_gt)

# Modelin doğruluğunu değerlendirmek için hata skoru hesaplayacağoz
# Tahmin edilen konumlar (res_df) ve gerçek konumlar (llh_gt) arasındaki hata metriğini kullanacağız bunu yaparken
score_nn = calc_score(res_df[:-1, :], llh_gt[:-1, :])

# Farklı yöntemlerle elde edilen hata skorları:
print(f'Referans Noktası (Baseline) Skoru: {score_bl:.4f} [metre]')
print(f'WLS Yöntemi Skoru: {score_wls:.4f} [metre]')
print(f'Kalman Filtresi Skoru: {score_kf:.4f} [metre]')
print(f'YSA (LSTM) Skoru:   {score_nn:.4f} [m]')

y

# Modelin Performansını Karşılaştıran Grafikler


plt.figure(figsize=(15, 5))
plt.plot(range(len(vd_bl)), vd_bl, label="Baseline")
plt.plot(range(len(vd_wls)), vd_wls, label="WLS")
plt.plot(range(len(vd_kf)), vd_kf, label="Kalman Filtresi")
plt.plot(range(len(vd_nn)), vd_nn, label="LSTM")

plt.xlabel("Zaman [sn]")
plt.ylabel("Hata Mesafesi [m]")
plt.title("Modellerin Hata Karşılaştırması")

plt.legend()
plt.show()